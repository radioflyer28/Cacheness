#!/usr/bin/env python3
"""
Quick backend comparison benchmark - focused demonstration of key tradeoffs.
"""

import time
import tempfile
import os
from cacheness import CacheConfig, cacheness
import numpy as np
import pandas as pd


def create_test_data(size_category="small"):
    """Create test data of different sizes."""
    if size_category == "small":
        return {
            "values": list(range(100)),
            "metadata": f"Size: {size_category}"
        }
    elif size_category == "medium":
        return {
            "array": np.random.random(1000),
            "dataframe": pd.DataFrame(np.random.random((100, 5))),
            "metadata": f"Size: {size_category}"
        }
    else:  # large
        return {
            "array": np.random.random(10000),
            "dataframe": pd.DataFrame(np.random.random((1000, 10))),
            "metadata": f"Size: {size_category}"
        }


def quick_benchmark():
    """Quick benchmark showing key tradeoffs."""
    print("üöÄ Quick Backend Comparison")
    print("=" * 50)
    
    # Test scenarios: small and large cache sizes
    scenarios = [
        ("Small cache (50 entries)", 50, "small"),
        ("Medium cache (200 entries)", 200, "small"),
        ("Large cache (1000 entries)", 1000, "small"),
    ]
    
    backends = ["memory", "json", "sqlite"]
    
    print(f"{'Scenario':<25} | {'Backend':<8} | {'PUT ops/sec':<12} | {'GET ops/sec':<12} | {'LIST time':<10}")
    print("-" * 85)
    
    for scenario_name, num_entries, data_size in scenarios:
        for backend in backends:
            with tempfile.TemporaryDirectory() as temp_dir:
                cache_dir = os.path.join(temp_dir, f"{backend}_cache")
                
                config = CacheConfig(
                    cache_dir=cache_dir,
                    metadata_backend=backend
                )
                cache = cacheness(config)
                
                # Generate test data
                test_data = create_test_data(data_size)
                
                # Benchmark PUT operations
                start = time.time()
                cache_keys = []
                for i in range(num_entries):
                    key = cache.put(test_data, test_id=i, scenario=scenario_name)
                    cache_keys.append(key)
                put_time = time.time() - start
                put_ops_per_sec = num_entries / put_time
                
                # Benchmark GET operations
                start = time.time()
                for key in cache_keys:
                    result = cache.get(cache_key=key)
                    assert result is not None
                get_time = time.time() - start
                get_ops_per_sec = num_entries / get_time
                
                # Benchmark metadata operations
                start = time.time()
                cache.list_entries()
                list_time = (time.time() - start) * 1000  # Convert to ms
                
                print(f"{scenario_name:<25} | {backend:<8} | {put_ops_per_sec:>8.0f} | {get_ops_per_sec:>8.0f} | {list_time:>8.1f}ms")


def demonstrate_concurrency_issue():
    """Demonstrate JSON backend concurrency limitations."""
    print("\nüîÑ Concurrency Demonstration")
    print("=" * 50)
    
    print("Testing what happens when multiple processes access the same cache...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        memory_cache_dir = os.path.join(temp_dir, "memory_cache")
        json_cache_dir = os.path.join(temp_dir, "json_cache")
        sqlite_cache_dir = os.path.join(temp_dir, "sqlite_cache")
        
        # Create all caches with some initial data
        cache_configs = [
            (memory_cache_dir, "memory"),
            (json_cache_dir, "json"), 
            (sqlite_cache_dir, "sqlite")
        ]
        
        for cache_dir, backend in cache_configs:
            config = CacheConfig(cache_dir=cache_dir, metadata_backend=backend)
            cache = cacheness(config)
            
            # Add some initial data
            for i in range(10):
                cache.put({"data": f"initial_{i}"}, setup=True, item=i)
        
        print("\nüöÄ Memory Backend:")
        print("  ‚úÖ  Ultra-fast O(1) operations")
        print("  ‚úÖ  Thread-safe with minimal locking")
        print("  ‚úÖ  Perfect for temporary high-performance caching")
        print("  ‚ö†Ô∏è  No persistence - data lost on restart")
        print("  ‚ö†Ô∏è  Limited to single process (no shared memory)")
        
        print("\nüìÅ JSON Backend:")
        print("  ‚ö†Ô∏è  No built-in concurrency protection")
        print("  ‚ö†Ô∏è  Multiple processes can corrupt metadata files")
        print("  ‚ö†Ô∏è  Race conditions during file writes")
        print("  ‚úÖ  Fast for single-process access")
        
        print("\nüíæ SQLite Backend:")
        print("  ‚úÖ  Built-in concurrency protection with WAL mode")
        print("  ‚úÖ  ACID transactions prevent corruption")
        print("  ‚úÖ  Handles multiple readers/writers safely")
        print("  ‚úÖ  Production-ready for multi-process apps")


def show_scaling_characteristics():
    """Show how performance scales with cache size."""
    print("\nüìà Scaling Characteristics")
    print("=" * 50)
    
    cache_sizes = [50, 200, 500, 1000]
    
    print("How list_entries() performance scales:")
    print(f"{'Cache Size':<12} | {'Memory (ms)':<12} | {'JSON (ms)':<10} | {'SQLite (ms)':<12} | {'Fastest':<10}")
    print("-" * 70)
    
    for size in cache_sizes:
        memory_time = json_time = sqlite_time = 0
        
        # Test Memory
        with tempfile.TemporaryDirectory() as temp_dir:
            cache_dir = os.path.join(temp_dir, "memory_cache")
            config = CacheConfig(cache_dir=cache_dir, metadata_backend="memory")
            cache = cacheness(config)
            
            # Add entries
            for i in range(size):
                cache.put({"data": f"entry_{i}"}, entry=i)
            
            # Measure list_entries
            start = time.time()
            cache.list_entries()
            memory_time = (time.time() - start) * 1000
        
        # Test JSON
        with tempfile.TemporaryDirectory() as temp_dir:
            cache_dir = os.path.join(temp_dir, "json_cache")
            config = CacheConfig(cache_dir=cache_dir, metadata_backend="json")
            cache = cacheness(config)
            
            # Add entries
            for i in range(size):
                cache.put({"data": f"entry_{i}"}, entry=i)
            
            # Measure list_entries
            start = time.time()
            cache.list_entries()
            json_time = (time.time() - start) * 1000
        
        # Test SQLite
        with tempfile.TemporaryDirectory() as temp_dir:
            cache_dir = os.path.join(temp_dir, "sqlite_cache")
            config = CacheConfig(cache_dir=cache_dir, metadata_backend="sqlite")
            cache = cacheness(config)
            
            # Add entries
            for i in range(size):
                cache.put({"data": f"entry_{i}"}, entry=i)
            
            # Measure list_entries
            start = time.time()
            cache.list_entries()
            sqlite_time = (time.time() - start) * 1000
        
        # Determine fastest
        times = [("Memory", memory_time), ("JSON", json_time), ("SQLite", sqlite_time)]
        fastest = min(times, key=lambda x: x[1])[0]
        
        print(f"{size:<12} | {memory_time:<12.1f} | {json_time:<10.1f} | {sqlite_time:<12.1f} | {fastest:<10}")


def main():
    """Run quick demonstration of backend tradeoffs."""
    print("üéØ Cacheness Backend Tradeoffs - Quick Demo")
    print("=" * 60)
    print("Demonstrating when to use JSON vs SQLite backends\n")
    
    quick_benchmark()
    demonstrate_concurrency_issue()
    show_scaling_characteristics()
    
    print("\n" + "=" * 60)
    print("üìã SUMMARY - When to use each backend:")
    print()
    print("ÔøΩ Memory Backend:")
    print("  ‚úÖ Fastest possible performance (O(1) operations)")
    print("  ‚úÖ Perfect for temporary high-frequency caching")
    print("  ‚úÖ Thread-safe with minimal overhead")
    print("  ‚úÖ Ideal for development and testing")
    print("  ‚ö†Ô∏è  No persistence - data lost on restart")
    print("  ‚ö†Ô∏è  Single process only (no shared memory)")
    print()
    print("ÔøΩüìÅ JSON Backend:")
    print("  ‚úÖ Faster for small caches (< 200 entries)")
    print("  ‚úÖ Simple setup and deployment")
    print("  ‚úÖ Human-readable metadata files")
    print("  ‚úÖ Great for development and testing")
    print("  ‚ö†Ô∏è  No concurrency protection")
    print("  ‚ö†Ô∏è  Performance degrades with cache size")
    print()
    print("üíæ SQLite Backend:")
    print("  ‚úÖ Scales well with large caches (> 200 entries)")
    print("  ‚úÖ Full concurrency support (multiple processes)")
    print("  ‚úÖ ACID transactions and data integrity")
    print("  ‚úÖ Production-ready and robust")
    print("  ‚ö†Ô∏è  Slightly slower for very small caches")
    print("  ‚ö†Ô∏è  More complex setup (requires SQLite)")
    print()
    print("üéØ RECOMMENDATION:")
    print("  ‚Ä¢ Temporary/High-performance: Memory backend")
    print("  ‚Ä¢ Development/Testing: JSON backend")
    print("  ‚Ä¢ Production/Multi-process: SQLite backend")
    print("  ‚Ä¢ Crossover point: ~200 cache entries")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Comprehensive Backend Performance Benchmark
===========================================

This consolidates multiple backend benchmarks into a single comprehensive test:
1. Raw backend performance comparison (Memory, JSON, SQLite)
2. Memory cache layer impact (enabled vs disabled for JSON/SQLite)
3. Realistic workload patterns with different access patterns
4. Scaling characteristics across different cache sizes
5. Concurrent access patterns

Replaces: backend_comparison_benchmark.py, test_entry_caching.py, 
         test_realistic_caching.py, quick_backend_demo.py, list_performance_analysis.py
"""

import time
import tempfile
import os
import random
import numpy as np
from cacheness import CacheConfig, cacheness


def create_test_data(size_category="small"):
    """Create test data of different sizes for realistic testing."""
    if size_category == "small":
        return {
            "array": np.random.random(50).tolist(),
            "metadata": {"category": size_category, "created": time.time()}
        }
    elif size_category == "medium": 
        return {
            "array": np.random.random(500).tolist(),
            "dataframe_data": np.random.random((20, 5)).tolist(),
            "metadata": {"category": size_category, "created": time.time()}
        }
    else:  # large
        return {
            "array": np.random.random(2000).tolist(),
            "dataframe_data": np.random.random((100, 10)).tolist(),
            "metadata": {"category": size_category, "created": time.time()}
        }


def benchmark_raw_backend_performance():
    """Test 1: Raw backend performance without memory cache layer."""
    print("üèÅ Test 1: Raw Backend Performance Comparison")
    print("=" * 80)
    print("Testing pure backend performance with memory cache layer DISABLED")
    print()
    
    backends = ["memory", "json", "sqlite"]
    cache_sizes = [50, 200, 500, 1000]
    
    print("Backend   | Cache Size | PUT ops/sec | GET ops/sec | LIST time (ms)")
    print("-" * 70)
    
    for backend in backends:
        for size in cache_sizes:
            with tempfile.TemporaryDirectory() as temp_dir:
                cache_dir = os.path.join(temp_dir, f"{backend}_test")
                
                config = CacheConfig(
                    cache_dir=cache_dir,
                    metadata_backend=backend,
                    max_cache_size_mb=5000,
                    # Disable memory cache layer for raw performance
                    enable_memory_cache=False,
                    memory_cache_stats=False
                )
                cache = cacheness(config)
                
                # Generate test data
                test_data = create_test_data("small")
                
                # Benchmark PUT operations
                start = time.time()
                cache_keys = []
                for i in range(size):
                    key = cache.put(test_data, test_id=i)
                    cache_keys.append(key)
                put_time = time.time() - start
                put_ops_per_sec = size / put_time
                
                # Benchmark GET operations
                start = time.time()
                for key in cache_keys:
                    result = cache.get(cache_key=key)
                    assert result is not None
                get_time = time.time() - start
                get_ops_per_sec = size / get_time
                
                # Benchmark LIST operations
                start = time.time()
                cache.list_entries()
                list_time = (time.time() - start) * 1000
                
                print(f"{backend:9} | {size:10d} | {put_ops_per_sec:8.0f}    | {get_ops_per_sec:8.0f}    | {list_time:8.1f}")


def benchmark_memory_cache_layer_impact():
    """Test 2: Memory cache layer performance impact for JSON and SQLite."""
    print("\n\nüöÄ Test 2: Memory Cache Layer Impact")
    print("=" * 80)
    print("Comparing JSON and SQLite backends with memory cache layer ON vs OFF")
    print()
    
    backends = ["json", "sqlite"]  # Memory backend doesn't use cache layer
    cache_size = 500
    num_operations = 1000
    
    print("Backend | Cache Layer | PUT ops/sec | GET ops/sec | Hit Rate | Cache Stats")
    print("-" * 85)
    
    for backend in backends:
        for cache_enabled in [False, True]:
            with tempfile.TemporaryDirectory() as temp_dir:
                cache_dir = os.path.join(temp_dir, f"{backend}_cache_test")
                
                config = CacheConfig(
                    cache_dir=cache_dir,
                    metadata_backend=backend,
                    max_cache_size_mb=5000,
                    enable_memory_cache=cache_enabled,
                    memory_cache_type="lru",
                    memory_cache_maxsize=100,
                    memory_cache_ttl_seconds=300,
                    memory_cache_stats=True
                )
                cache = cacheness(config)
                
                # Populate cache first
                test_data = create_test_data("medium")
                cache_keys = []
                for i in range(cache_size):
                    key = cache.put(test_data, test_id=i)
                    cache_keys.append(key)
                
                # Benchmark repeated GET operations (80% hot data pattern)
                hot_keys = cache_keys[:int(cache_size * 0.2)]  # 20% of keys are "hot"
                
                start = time.time()
                for _ in range(num_operations):
                    # 80% chance to access hot data, 20% chance to access cold data
                    if random.random() < 0.8:
                        key = random.choice(hot_keys)
                    else:
                        key = random.choice(cache_keys)
                    result = cache.get(cache_key=key)
                    assert result is not None
                
                get_time = time.time() - start
                get_ops_per_sec = num_operations / get_time
                
                # Get cache statistics
                stats = cache.get_stats()
                cache_hit_rate = stats.get('memory_cache_hit_rate', 'N/A')
                cache_size_info = f"{stats.get('memory_cache_size', 'N/A')}/{stats.get('memory_cache_maxsize', 'N/A')}"
                
                cache_status = "ENABLED " if cache_enabled else "DISABLED"
                print(f"{backend:7} | {cache_status:11} | {'N/A':8}     | {get_ops_per_sec:8.0f}    | {cache_hit_rate:8}  | {cache_size_info}")


def benchmark_realistic_workload_patterns():
    """Test 3: Realistic workload patterns with different access distributions."""
    print("\n\nüìä Test 3: Realistic Workload Patterns")
    print("=" * 80)
    print("Testing different access patterns: Sequential, Random, Hot-Spot (80/20 rule)")
    print()
    
    patterns = [
        ("Sequential", lambda keys, n: [keys[i % len(keys)] for i in range(n)]),
        ("Random", lambda keys, n: [random.choice(keys) for _ in range(n)]),
        ("Hot-Spot", lambda keys, n: [
            random.choice(keys[:len(keys)//5]) if random.random() < 0.8 
            else random.choice(keys) for _ in range(n)
        ])
    ]
    
    cache_size = 1000
    num_operations = 2000
    
    print("Pattern     | Backend | Cache Layer | ops/sec | Cache Hit Rate")
    print("-" * 60)
    
    for pattern_name, pattern_func in patterns:
        for backend in ["json", "sqlite"]:
            for cache_enabled in [False, True]:
                with tempfile.TemporaryDirectory() as temp_dir:
                    cache_dir = os.path.join(temp_dir, f"pattern_test_{backend}")
                    
                    config = CacheConfig(
                        cache_dir=cache_dir,
                        metadata_backend=backend,
                        max_cache_size_mb=5000,
                        enable_memory_cache=cache_enabled,
                        memory_cache_type="lru",
                        memory_cache_maxsize=200,  # 20% of cache size
                        memory_cache_ttl_seconds=300,
                        memory_cache_stats=True
                    )
                    cache = cacheness(config)
                    
                    # Populate cache
                    test_data = create_test_data("small")
                    cache_keys = []
                    for i in range(cache_size):
                        key = cache.put(test_data, test_id=i)
                        cache_keys.append(key)
                    
                    # Generate access pattern
                    access_keys = pattern_func(cache_keys, num_operations)
                    
                    # Benchmark the pattern
                    start = time.time()
                    for key in access_keys:
                        result = cache.get(cache_key=key)
                        assert result is not None
                    
                    pattern_time = time.time() - start
                    ops_per_sec = num_operations / pattern_time
                    
                    # Get cache statistics
                    stats = cache.get_stats()
                    hit_rate = stats.get('memory_cache_hit_rate', 'N/A')
                    
                    cache_status = "ON " if cache_enabled else "OFF"
                    print(f"{pattern_name:11} | {backend:7} | {cache_status:11} | {ops_per_sec:7.0f} | {hit_rate}")


def benchmark_scaling_characteristics():
    """Test 4: How backends scale with increasing cache sizes."""
    print("\n\nüìà Test 4: Backend Scaling Characteristics")
    print("=" * 80)
    print("Testing how list_entries() performance scales with cache size")
    print()
    
    sizes = [10, 50, 100, 500, 1000, 2000]
    backends = ["memory", "json", "sqlite"]
    
    print("Cache Size | Memory (ms) | JSON (ms)   | SQLite (ms) | SQLite/JSON Ratio")
    print("-" * 75)
    
    for size in sizes:
        results = {}
        
        for backend in backends:
            with tempfile.TemporaryDirectory() as temp_dir:
                cache_dir = os.path.join(temp_dir, f"scale_test_{backend}")
                
                config = CacheConfig(
                    cache_dir=cache_dir,
                    metadata_backend=backend,
                    max_cache_size_mb=10000,
                    enable_memory_cache=False  # Test raw performance
                )
                cache = cacheness(config)
                
                # Populate cache
                test_data = create_test_data("small")
                for i in range(size):
                    cache.put(test_data, test_id=i)
                
                # Time list operation
                start = time.time()
                cache.list_entries()
                list_time = (time.time() - start) * 1000
                
                results[backend] = list_time
        
        # Calculate ratio
        ratio = results["sqlite"] / results["json"] if results["json"] > 0 else 0
        
        print(f"{size:10d} | {results['memory']:8.1f}    | {results['json']:8.1f}    | {results['sqlite']:8.1f}     | {ratio:8.1f}x")


def benchmark_memory_cache_effectiveness():
    """Test 5: Detailed memory cache effectiveness analysis."""
    print("\n\nüéØ Test 5: Memory Cache Effectiveness Analysis")
    print("=" * 80)
    print("Detailed analysis of memory cache layer benefits for repeated operations")
    print()
    
    backend = "sqlite"  # Use SQLite as it benefits most from caching
    cache_size = 500
    
    for cache_enabled in [False, True]:
        cache_status = "ENABLED" if cache_enabled else "DISABLED"
        print(f"\nüîç Memory Cache {cache_status}:")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            cache_dir = os.path.join(temp_dir, "effectiveness_test")
            
            config = CacheConfig(
                cache_dir=cache_dir,
                metadata_backend=backend,
                max_cache_size_mb=5000,
                enable_memory_cache=cache_enabled,
                memory_cache_type="lru",
                memory_cache_maxsize=100,
                memory_cache_ttl_seconds=300,
                memory_cache_stats=True
            )
            cache = cacheness(config)
            
            # Populate cache
            test_data = create_test_data("medium")
            cache_keys = []
            for i in range(cache_size):
                key = cache.put(test_data, test_id=i)
                cache_keys.append(key)
            
            # Test repeated get_entry calls on same keys
            test_keys = cache_keys[:50]  # Test with subset of keys
            
            times = []
            for round_num in range(5):
                start = time.time()
                for key in test_keys:
                    result = cache.get(cache_key=key)
                    assert result is not None
                round_time = (time.time() - start) * 1000
                times.append(round_time)
                print(f"   Round {round_num + 1}: {round_time:6.1f}ms ({len(test_keys)} gets)")
            
            # Show statistics
            avg_time = sum(times) / len(times)
            first_vs_avg = times[0] / avg_time if avg_time > 0 else 1
            
            print(f"   Average: {avg_time:6.1f}ms")
            print(f"   First vs Avg ratio: {first_vs_avg:4.1f}x")
            
            if cache_enabled:
                stats = cache.get_stats()
                hit_rate = stats.get('memory_cache_hit_rate', 0)
                cache_size_used = stats.get('memory_cache_size', 0)
                print(f"   Cache hit rate: {hit_rate:.3f}")
                print(f"   Cache utilization: {cache_size_used}/100")


def main():
    """Run comprehensive backend benchmark suite."""
    print("üèÜ Cacheness Comprehensive Backend Benchmark")
    print("=" * 80)
    print("Testing all backend types and memory cache layer configurations")
    print("This may take several minutes to complete...")
    print()
    
    try:
        benchmark_raw_backend_performance()
        benchmark_memory_cache_layer_impact() 
        benchmark_realistic_workload_patterns()
        benchmark_scaling_characteristics()
        benchmark_memory_cache_effectiveness()
        
        print("\n\nüéØ Summary and Recommendations")
        print("=" * 80)
        print("üìã Backend Selection Guide:")
        print("‚Ä¢ Memory Backend: Ultra-fast, no persistence, single-process only")
        print("‚Ä¢ JSON Backend: Good for small caches (<500 entries), single-process apps")
        print("‚Ä¢ SQLite Backend: Best for large caches (>500 entries), production apps")
        print()
        print("üöÄ Memory Cache Layer:")
        print("‚Ä¢ Significant benefit for SQLite backend (database overhead reduction)")
        print("‚Ä¢ Moderate benefit for JSON backend (file I/O reduction)")
        print("‚Ä¢ Not applicable to Memory backend (already in-memory)")
        print()
        print("‚ö° Performance Patterns:")
        print("‚Ä¢ Hot-spot access patterns benefit most from memory cache layer")
        print("‚Ä¢ Sequential access has minimal caching benefit")
        print("‚Ä¢ Random access shows moderate caching benefit")
        print()
        print("‚úÖ Benchmark complete!")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Benchmark interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Benchmark failed: {e}")


if __name__ == "__main__":
    main()
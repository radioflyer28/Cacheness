{"id":"CACHE-01h","title":"Create utils.py module or remove docs references","description":"API_REFERENCE.md documents `hash_content()`, `hash_file_path()`, `get_size_mb()` as utility functions and `migrate_cache()` from `cacheness.utils` — but `utils.py` doesn't exist.\n\nSome of these functions exist internally (e.g., in `file_hashing.py`) but aren't exported as public utilities.","design":"1. Create `src/cacheness/utils.py` if implementing\n2. `hash_content`, `hash_file_path` can wrap existing `file_hashing.py` internals\n3. `get_size_mb` can wrap `os.path.getsize` with MB conversion\n4. `migrate_cache(source_backend, target_backend)` copies entries between backends\n5. Alternative: just clean up docs to stop referencing non-existent module","acceptance_criteria":"- [ ] Either: `utils.py` created with `hash_content`, `hash_file_path`, `get_size_mb`, `migrate_cache`\n- [ ] Or: Documentation references removed from API_REFERENCE.md\n- [ ] If implemented: functions exported in `__init__.py`\n- [ ] If implemented: `migrate_cache()` provides backend migration capability\n- [ ] Decision documented\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":4,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:58.1398206-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:58.1398206-05:00","labels":["api","documentation","low"],"dependencies":[{"issue_id":"CACHE-01h","depends_on_id":"CACHE-6pp","type":"related","created_at":"2026-02-08T13:03:13.1432507-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-0vh","title":"Simplify signing: remove actual_path, eliminate custom_signed_fields, wire up signature_version","description":"Simplify the signing subsystem based on review findings.\n\n**Problem:**\n- `actual_path` in signed fields is redundant (file_hash covers content) and harms portability\n- `custom_signed_fields` has no practical use case — signing is an internal concern, not user-configurable\n- `signature_version` exists in SecurityConfig but is never stored or used\n- `description` is extracted in `_extract_signable_fields()` but never actually signed (dead weight)\n\n**Changes:**\n\n1. **Remove `actual_path` from default signed fields** in security.py\n2. **Remove `custom_signed_fields`** from SecurityConfig and CacheEntrySigner — library decides the right fields\n3. **Remove `VALID_SIGNED_FIELDS`** set from SecurityConfig (no longer needed)\n4. **Make `signature_version` internal** — remove from SecurityConfig, manage inside CacheEntrySigner\n5. **Store `signature_version` in metadata** alongside `entry_signature` during put (all paths)\n6. **Read stored version during verify** — use `SIGNED_FIELDS_BY_VERSION` mapping in CacheEntrySigner\n7. **Version mapping:**\n   - v1: original field list (with actual_path) — for legacy entry compat\n   - v2: actual_path removed, description removed from extraction\n8. **Legacy fallback:** entries without stored `signature_version` → assume v1\n9. **Remove dead `description` extraction** from `_extract_signable_fields()`\n10. **Consider:** always compute file_hash when signing is enabled (regardless of verify_cache_integrity)\n\n**SecurityConfig after cleanup:**\n- `enable_entry_signing: bool` (on/off)\n- `signing_key_file: str` (key path)\n- `use_in_memory_key: bool` (testing)\n- `allow_unsigned_entries: bool` (migration)\n- `delete_invalid_signatures: bool` (behavior)\n\n**Removed from SecurityConfig:**\n- `custom_signed_fields` — no use case\n- `signature_version` — internal to CacheEntrySigner\n- `VALID_SIGNED_FIELDS` — validation set no longer needed","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T12:45:50.4204704-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T14:39:27.4287918-05:00","closed_at":"2026-02-12T14:39:27.4287918-05:00","close_reason":"Implemented: version embedded in signature string (v2:hex), removed custom_signed_fields, removed actual_path from v2 signed fields. 886 tests pass.","labels":["security","signing"]}
{"id":"CACHE-1a8","title":"Implement @cache_if conditional caching decorator","description":"Documented in API_REFERENCE.md L832 but not implemented. Users often want to cache only successful results (e.g., only cache non-None API responses, only cache DataFrames with \u003e0 rows).","design":"1. Add `cache_if` class to decorators.py\n2. Accept `condition` parameter — a callable `(result) -\u003e bool`\n3. In `__call__`, after computing the result, check `condition(result)` before storing\n4. If condition is False, return result without caching\n5. Cache lookup still happens normally (check cache first, compute if miss)","acceptance_criteria":"- [ ] `@cache_if(predicate)` decorator conditionally caches based on return value\n- [ ] Predicate receives the function's return value: `predicate(result) -\u003e bool`\n- [ ] If predicate returns `False`, result is returned but NOT cached\n- [ ] Supports same parameters as `@cached` (ttl, prefix, etc.)\n- [ ] Test: Only cache when predicate is True (e.g., cache only non-None results)\n- [ ] Test: Predicate receives the actual return value\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":3,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.0438306-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T17:38:29.5679813-05:00","closed_at":"2026-02-09T17:38:29.5679813-05:00","close_reason":"Completed - @cache_if decorator implemented with 9 comprehensive tests, all tests pass (821 passed)","labels":["api","decorators","medium"],"dependencies":[{"issue_id":"CACHE-1a8","depends_on_id":"CACHE-869","type":"related","created_at":"2026-02-08T13:03:08.2577876-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-1kn","title":"Implement cache_clear() on @cached decorator","description":"decorators.py L224 — `_clear_cache()` is a stub that always returns 0 without deleting anything. The comment says: \"For now, we'll return 0 as we don't have pattern-based deletion.\" But `delete_matching()` now exists, so this can be implemented.\n\nUsers calling `my_func.cache_clear()` get a silently wrong result — they believe 0 entries were deleted when entries still exist.","design":"1. The `@cached` decorator already has `key_prefix` — use this to scope entries per function\n2. If `key_prefix` is set, use `cache.delete_matching(prefix=key_prefix)` or `cache.delete_where(prefix=key_prefix)`\n3. If no `key_prefix`, fall back to function qualname as prefix\n4. Track whether the `CacheDecorator` has a cache instance and call its deletion methods\n5. Return actual count of deleted entries","acceptance_criteria":"- [ ] `cache_clear()` actually deletes all entries created by the decorated function\n- [ ] Returns the count of deleted entries\n- [ ] Uses prefix-based or pattern-based deletion\n- [ ] Thread-safe\n- [ ] Test: Decorate function, create entries, call `cache_clear()`, verify entries are gone\n- [ ] Test: `cache_clear()` on one function doesn't affect another function's entries\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":2,"issue_type":"bug","assignee":"GitHub Copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:01:16.9040675-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T12:45:49.4983967-05:00","closed_at":"2026-02-09T12:45:49.4983967-05:00","close_reason":"Feature complete: cache_clear() now tracks cache keys and actually deletes entries. 4 comprehensive tests added. Merged to dev.","labels":["api","decorators","high"],"dependencies":[{"issue_id":"CACHE-1kn","depends_on_id":"CACHE-6li","type":"related","created_at":"2026-02-08T13:03:08.2588261-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-1nz","title":"Expose cleanup_expired() as public method","description":"Documented in API_REFERENCE.md L280 as a public method, but only exists as `_cleanup_expired` (private, core.py L784) called during `__init__`. Users need to trigger TTL cleanup on demand for long-running applications without restarting the cache.","design":"1. Add public `cleanup_expired(ttl_seconds=None) -\u003e int` method\n2. Delegate to existing `_cleanup_expired()` logic\n3. Default `ttl_seconds` to `self.config.storage.default_ttl_seconds`\n4. Use `self._lock` for thread safety\n5. Ensure blob files are deleted alongside metadata","acceptance_criteria":"- [ ] `cleanup_expired(ttl_seconds=None) -\u003e int` added as public method on `UnifiedCache`\n- [ ] Defaults to `config.storage.default_ttl_seconds` if no argument provided\n- [ ] Returns count of entries removed\n- [ ] Deletes both metadata and blob files for expired entries\n- [ ] Thread-safe (uses `self._lock`)\n- [ ] Test: Create entries, advance time, call `cleanup_expired()`, verify removal\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:01:16.9135815-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T12:06:56.5683061-05:00","closed_at":"2026-02-09T12:06:56.5683061-05:00","close_reason":"Feature complete: cleanup_expired() exposed as public method with 4 comprehensive tests. Removes both metadata and blob files. Merged to dev.","labels":["api","core","high"]}
{"id":"CACHE-1y3","title":"Add __len__, __contains__, __iter__ dunder methods","description":"UnifiedCache has no collection-style dunder methods. Users can't use Pythonic patterns like `len(cache)`, `key in cache`, or `for entry in cache`. These are natural expectations for a cache object.\n\nDepends on `exists()` being implemented first for `__contains__`.","design":"```python\ndef __len__(self) -\u003e int:\n    with self._lock:\n        stats = self.metadata_backend.get_stats()\n        return stats.get(\"total_entries\", 0)\n\ndef __contains__(self, cache_key: str) -\u003e bool:\n    return self.exists(cache_key=cache_key)\n\ndef __iter__(self):\n    with self._lock:\n        yield from self.metadata_backend.iter_entry_summaries()\n```","acceptance_criteria":"- [ ] `__len__` returns total entry count\n- [ ] `__contains__` checks entry existence by cache key (metadata-only)\n- [ ] `__iter__` iterates over entry summaries\n- [ ] All three are thread-safe\n- [ ] Test: `len(cache)` returns correct count\n- [ ] Test: `\"key\" in cache` works for existing and missing keys\n- [ ] Test: `for entry in cache` yields all entries\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":3,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.0788736-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:17.0788736-05:00","labels":["api","core","medium"],"dependencies":[{"issue_id":"CACHE-1y3","depends_on_id":"CACHE-6li","type":"blocks","created_at":"2026-02-08T13:03:08.224178-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-2sf","title":"storage_mode becomes BlobStore passthrough","description":"With composition in place, storage_mode=True on UnifiedCache simply exposes BlobStore directly without cache layer overhead. No TTL, eviction, stats wrapping. Methods delegate straight to BlobStore. Phase 1 storage_mode guards become composition-based rather than if-checks.","status":"closed","priority":2,"issue_type":"task","assignee":"copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-11T16:56:02.1836372-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T11:56:38.0728289-05:00","closed_at":"2026-02-12T11:56:38.0728289-05:00","close_reason":"Implemented: storage_mode now early-returns to dedicated passthrough methods (_storage_mode_put, _storage_mode_get, _storage_mode_get_with_metadata) that bypass TTL/eviction/stats/auto-delete. Removed 8 scattered if-guards from normal paths. 884 passed, 65 skipped. Merged to dev at 923cf2c.","labels":["phase-2","refactor","storage-mode"],"dependencies":[{"issue_id":"CACHE-2sf","depends_on_id":"CACHE-5lm","type":"blocks","created_at":"2026-02-11T16:56:13.1867344-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-2sf","depends_on_id":"CACHE-i4b","type":"parent-child","created_at":"2026-02-11T16:56:13.2500486-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-2tz","title":"Build wheels and publish to GitHub Releases","description":"Set up CI workflow to build wheels using uv_build backend and publish to GitHub Releases.\\n\\n1. Update build-system requires to `uv_build\u003e=0.10.2,\u003c0.11.0` (per current uv docs recommendation)\\n2. Create GitHub Actions workflow (.github/workflows/release.yml) that:\\n   - Triggers on version tag push (v*)\\n   - Builds sdist + wheel using `uv build`\\n   - Creates GitHub Release with built artifacts\\n   - Optionally publishes to PyPI\\n3. Verify `uv build` produces correct wheel with `src/cacheness/` module\\n4. Add build instructions to docs\\n\\nRef: https://docs.astral.sh/uv/concepts/build-backend/","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T15:27:29.9529808-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T16:34:11.2467744-05:00","closed_at":"2026-02-12T16:34:11.2467744-05:00","close_reason":"Closed","labels":["ci","packaging"]}
{"id":"CACHE-32o","title":"Storage-mode signing gap: _storage_mode_put skips signing, get rejects unsigned","description":"_storage_mode_put() never signs entries (by design), but _storage_mode_get() verifies signatures. When configured with enable_entry_signing=True + allow_unsigned_entries=False, every storage-mode entry silently fails to load (returns None with a warning). Fix: add signing enrichment to _storage_mode_put() (~10 lines) so entries get signed like the normal put() path.","status":"closed","priority":2,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T12:18:49.4517046-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T12:33:12.7628601-05:00","closed_at":"2026-02-12T12:33:12.7628601-05:00","close_reason":"Fixed: _storage_mode_put() now signs entries via signer.sign_entry(). Added 4 tests in TestStorageModeSigning. Committed 6545cb7, pushed to dev.","labels":["security","storage-mode"]}
{"id":"CACHE-4el","title":"Fix stale references in copilot-instructions.md","description":"copilot-instructions.md references `MemoryCacheWrapper` in the architecture diagram, but the actual class is `CachedMetadataBackend` at metadata.py L350. Also, the instructions say `_lock` is \"created but never acquired\" — this was fixed (all public methods now use `with self._lock`).","design":"1. Replace `MemoryCacheWrapper` with `CachedMetadataBackend` in architecture diagram\n2. Update the \"IMPORTANT\" note about `_lock` — it IS now acquired via `with self._lock`\n3. Review other potentially stale references","acceptance_criteria":"- [ ] `MemoryCacheWrapper` reference replaced with `CachedMetadataBackend` in copilot-instructions.md\n- [ ] Architecture diagram updated\n- [ ] Any other stale references corrected\n- [ ] Thread safety section accurate","status":"open","priority":4,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:58.1813382-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:58.1813382-05:00","labels":["documentation","low"]}
{"id":"CACHE-52z","title":"Replace eval() with ast.literal_eval() in array handler","description":"handlers.py L575 uses `eval(shape_str)` to parse a stored NumPy shape tuple from blosc2-compressed files. A crafted cache file could inject arbitrary Python code via the shape string (e.g., `\"__import__('os').system('rm -rf /')\"` instead of `\"(100, 200)\"`).\n\nThis is an arbitrary code execution vulnerability. While exploiting it requires write access to cache files, it violates defense-in-depth and is trivially fixable.","design":"1. Replace `eval(shape_str)` with `ast.literal_eval(shape_str)` at handlers.py L575\n2. `ast.literal_eval` only evaluates Python literals (tuples, lists, strings, numbers) and rejects arbitrary expressions\n3. Add `import ast` at top of handlers.py if not already present\n4. Verify the shape tuple format `\"(100, 200)\"` is parseable by `ast.literal_eval`","acceptance_criteria":"- [ ] `eval(shape_str)` replaced with `ast.literal_eval(shape_str)` at handlers.py L575\n- [ ] `import ast` added to handlers.py\n- [ ] Test: Verify normal shape tuples still parse correctly\n- [ ] Test: Verify malicious shape strings are rejected (raise ValueError)\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":1,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:00:28.8957123-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:36:05.9787215-05:00","closed_at":"2026-02-08T13:36:05.9787215-05:00","close_reason":"Completed - replaced eval() with ast.literal_eval() for security. All tests passing.","labels":["critical","handlers","security"]}
{"id":"CACHE-5lm","title":"Refactor UnifiedCache to compose BlobStore for storage ops","description":"Delegate storage operations (file I/O, metadata CRUD, handler dispatch) from UnifiedCache to BlobStore. UnifiedCache retains cache-specific concerns: TTL, eviction, stats, decorators. Direct file I/O in core.py replaced with BlobStore calls. BlobBackend abstraction comes for free via composition.","status":"closed","priority":2,"issue_type":"task","assignee":"copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-11T16:56:02.1795464-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T11:26:03.2592692-05:00","closed_at":"2026-02-12T11:26:03.2592692-05:00","close_reason":"Completed. UnifiedCache now composes BlobStore for storage delegation. verify_integrity, invalidate, clear_all, put file I/O, get reads, and _calculate_file_hash all delegate to BlobStore. Net -92 lines. 884 tests pass.","labels":["phase-2","refactor","storage-mode"],"dependencies":[{"issue_id":"CACHE-5lm","depends_on_id":"CACHE-i4b","type":"parent-child","created_at":"2026-02-11T16:56:13.2618958-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-5lm","depends_on_id":"CACHE-xf3","type":"blocks","created_at":"2026-02-11T16:56:13.3828566-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-69y","title":"Unify blob I/O path and wire blob backend registry","description":"During CACHE-6sp we wired FilesystemBlobBackend into BlobStore for delete/exists, but put/get still route through handlers doing direct filesystem I/O. This creates a split where:\n\n- Write/Read: handler.put(data, file_path) -\u003e direct Path I/O (parquet, npz, pkl)\n- Delete/Exists: blob_backend.delete_blob(path) / blob_backend.exists(path)\n\nIf a user plugs in S3BlobBackend, delete/exists go to S3 but put/get still write locally via handlers. This needs resolution: either handlers accept a blob_backend for I/O, or the split is explicitly documented as intentional (handlers manage format serialization to local paths, blob_backend manages lifecycle).\n\nAdditionally, the blob backend registry (register_blob_backend, get_blob_backend, list_blob_backends) is exported in __init__.py and __all__ with tests, but nothing in any data path uses it. BlobStore constructs FilesystemBlobBackend directly. The registry should either be wired into BlobStore.__init__ (accept string names) or demoted/removed.","design":"Options (pick one):\n\nOption A - Document the split as intentional:\n1. Handlers own format-specific serialization to LOCAL paths (by design for user extensibility)\n2. BlobBackend manages lifecycle (delete, exists) and could manage replication/sync\n3. Document clearly that remote storage (S3) requires a handler that knows about remote, not just a blob backend swap\n4. Demote/remove the unused registry or mark as experimental\n\nOption B - Full abstraction:\n1. BlobBackend interface gains write_from_handler/read_to_handler methods\n2. Handlers serialize to bytes/stream, blob backend handles storage location\n3. Wire registry into BlobStore constructor\n4. Higher risk - changes handler interface contract\n\nRecommendation: Option A (lower risk, matches user's design intent from 6sp discussion)","acceptance_criteria":"- [ ] Decision documented: handlers own local I/O vs full blob backend abstraction\n- [ ] Architecture docs updated with clear separation of concerns diagram\n- [ ] Blob backend registry either wired into BlobStore or demoted with rationale\n- [ ] BACKEND_SELECTION.md updated with blob backend vs metadata backend distinction\n- [ ] All existing tests pass (822 passed baseline)","status":"in_progress","priority":2,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-09T18:33:00.8037905-05:00","created_by":"radioflyer28","updated_at":"2026-02-10T09:08:45.9941173-05:00","labels":["architecture","blob-store","medium"]}
{"id":"CACHE-6ar","title":"Review metadata entry signing strategy","description":"Review whether partial-field signing vs. full-entry signing is the right approach for metadata entries (SQL rows or JSON entries).\n\nCurrent state: Only certain fields are signed rather than the entire entry.\n\nKey questions:\n- Are all critical fields included in the signature?\n- Is the signing logic consistently reproducible between entry creation and validation?\n- Would signing the entire entry be simpler/more reliable without performance penalty?\n- Are there any fields that should NOT be signed (e.g., mutable metadata)?\n\nThis affects both security.py signing logic and metadata backend validation.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T12:07:31.4335387-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T14:41:21.9964999-05:00","closed_at":"2026-02-12T14:41:21.9964999-05:00","close_reason":"Completed via CACHE-0vh: reviewed and simplified signing strategy — versioned signatures, removed custom_signed_fields, removed actual_path from v2.","labels":["metadata","security"]}
{"id":"CACHE-6j7","title":"Advanced Testing Framework (TigerBeetle-Inspired)","description":"Adopt targeted testing techniques inspired by TigerBeetle's testing philosophy — \"prove correctness, don't just demonstrate it\" — adapted to Cacheness's risk profile as a single-process caching library.\n\nAnalysis identified 7 untested failure modes:\n- _lock never acquired — put() not thread-safe at coordination layer\n- Same-key concurrent put() can corrupt metadata or produce orphaned blobs\n- Crash between blob write and metadata write → orphaned blobs\n- No blob cleanup on put() exception after blob is written\n- get() auto-deletes entries on transient I/O errors\n- get() auto-deletes on any handler exception\n- JSON backend silently discards all data on corruption\n\nFour workstreams address these gaps (~8-12 days total effort):\n1. Property-based testing with Hypothesis (P1)\n2. Fault injection via mocking (P1)\n3. Invariant checks / fsck() method (P2)\n4. Fix _lock + concurrency stress tests (P2)\n\nSkipped (cost too high for caching library risk profile):\n- Deterministic simulation (VOPR-style) — overkill for single-process lib\n- Continuous fuzzing infrastructure (CFO) — blast radius too low to justify\n\nReference: https://deepwiki.com/tigerbeetle/tigerbeetle/5.2-testing-and-simulation","status":"closed","priority":2,"issue_type":"epic","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:04:43.126371-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T15:03:24.1570314-05:00","closed_at":"2026-02-07T15:03:24.1570314-05:00","close_reason":"Epic is a tracking umbrella; child issues are ready to work independently. CACHE-78r (Hypothesis) already completed.","labels":["reliability","testing"]}
{"id":"CACHE-6li","title":"Implement exists() method on UnifiedCache","description":"Documented in API_REFERENCE.md L155 but not implemented on `UnifiedCache`. Users must currently call `get()` (which loads the entire blob into memory) just to check if a cache key exists. This is wasteful for large cached objects (DataFrames, NumPy arrays).\n\nAn `exists()` method should do a metadata-only check via `metadata_backend.get_entry()` without touching the blob file.","design":"```python\ndef exists(self, cache_key=None, prefix=\"\", **kwargs) -\u003e bool:\n    with self._lock:\n        if cache_key is None:\n            cache_key = self._create_cache_key(kwargs)\n        entry = self.metadata_backend.get_entry(cache_key)\n        if entry is None:\n            return False\n        # Check TTL expiration\n        if self.config.storage.default_ttl_seconds:\n            created = entry.get('created_at')\n            if created and (time.time() - created) \u003e self.config.storage.default_ttl_seconds:\n                return False\n        return True\n```","acceptance_criteria":"- [ ] `exists(cache_key=None, prefix=\"\", **kwargs) -\u003e bool` added to `UnifiedCache`\n- [ ] Metadata-only check — does NOT load the blob file\n- [ ] Supports both direct `cache_key` and kwargs-based lookup\n- [ ] Respects TTL — expired entries return `False`\n- [ ] Thread-safe (uses `self._lock`)\n- [ ] Test: `exists()` returns True for cached entry\n- [ ] Test: `exists()` returns False for missing entry\n- [ ] Test: `exists()` returns False for expired entry\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":2,"issue_type":"feature","assignee":"GitHub Copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:01:16.9151533-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T15:34:56.6040618-05:00","closed_at":"2026-02-08T15:34:56.6040618-05:00","close_reason":"Feature complete: exists() method implemented with 5 comprehensive tests. All tests passing (793 passed). Merged to dev and pushed.","labels":["api","core","high"]}
{"id":"CACHE-6pp","title":"Fix API_REFERENCE.md to match actual implementation","description":"API_REFERENCE.md documents ~10 features that don't exist in code: `exists()`, `get_with_metadata()`, `invalidate_by_cache_key()`, `@cache_if`, `@cache_async`, `set_default_cache()`, `migrate_cache()`, environment variables, utility functions. This misleads users who read the docs then find missing methods.\n\nAlso, several implemented features aren't documented: `query_meta()`, `verify_integrity()`, `query_custom()`.\n\nCONFIGURATION.md uses field names that don't match actual config class fields.","design":"1. Audit every API_REFERENCE.md entry against actual code\n2. Mark unimplemented features with \"Planned\" status\n3. Add documentation for implemented-but-undocumented features\n4. Fix CONFIGURATION.md field names to match CacheConfig\n5. Remove or update S3 example that uses invalid config API\n6. This should be updated incrementally as features are implemented","acceptance_criteria":"- [ ] All documented-but-missing features marked as \"Planned\" or removed\n- [ ] `invalidate_by_cache_key()` docs updated to reference `invalidate(cache_key=...)`\n- [ ] `clear()` docs consistent with implementation (alias for `clear_all()`)\n- [ ] Undocumented public methods (`query_meta`, `verify_integrity`) added to docs\n- [ ] Environment variable section accurate (either implemented or marked planned)\n- [ ] Config field names match actual CacheConfig fields\n- [ ] No examples reference non-existent API","status":"in_progress","priority":3,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.0749922-05:00","created_by":"radioflyer28","updated_at":"2026-02-10T10:19:31.2212052-05:00","labels":["documentation","medium"],"dependencies":[{"issue_id":"CACHE-6pp","depends_on_id":"CACHE-6li","type":"related","created_at":"2026-02-08T13:03:13.0604127-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-6pp","depends_on_id":"CACHE-jts","type":"related","created_at":"2026-02-08T13:03:13.0619608-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-6sp","title":"Refactor: Separate blob storage from metadata backends","description":"Architecture review reveals blob store code residing in compress_pickle.py and handlers mixing storage concerns. Need to enforce clean separation: metadata backends (JSON/SQLite/Postgres) should only store metadata index, blob backends (Filesystem/S3) should handle actual data storage.","design":"1. Extract file I/O from compress_pickle.py into FilesystemBlobBackend\n2. Create blob backend abstraction layer (already partially exists in storage/backends/blob_backends.py)\n3. Update handlers to use blob backends instead of direct compress_pickle calls\n4. Ensure metadata backends ONLY store: cache keys, timestamps, file paths/URLs, NOT actual blob operations\n5. BlobStore should delegate all storage to blob backends, not implement file I/O directly\n6. Review SQL backends to ensure they don't contain S3/file logic - they reference blob paths only","acceptance_criteria":"Clean separation verified: compress_pickle.py has no file I/O, metadata backends store only metadata, handlers use blob backends, BlobStore delegates to backends, SQL backends reference blob paths not storage logic","status":"closed","priority":2,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-09T17:45:07.1214772-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T18:26:44.9736125-05:00","closed_at":"2026-02-09T18:26:44.9736125-05:00","close_reason":"Closed","labels":["architecture","backend","refactoring"]}
{"id":"CACHE-6z0","title":"Run quality gates (ruff format, ruff check, ty) on codebase","description":"Run code quality and type checking tools before finalizing any work session. This should be a standard step in the development workflow.\n\nCommands to run:\n1. `uv run ruff format .` - Apply code formatting\n2. `uv run ruff check --fix .` - Fix linting issues automatically where possible\n3. `uv run ty` - Run type checking\n\nThese should be run from the main worktree (or dev worktree) after all feature branches have been merged, as a final quality check before considering work complete.","status":"closed","priority":4,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T14:29:09.4353518-05:00","created_by":"radioflyer28","updated_at":"2026-02-10T09:06:53.8139126-05:00","closed_at":"2026-02-10T09:06:53.8139126-05:00","close_reason":"Duplicate of CACHE-m3w"}
{"id":"CACHE-78r","title":"Property-Based Testing with Hypothesis","description":"Add property-based testing using Hypothesis to find edge cases in serialization round-trips, cache key determinism, and metadata backend contracts. Hypothesis is already mentioned in dev deps but is not installed or used anywhere.\n\nCurrent state: 0 property-based tests. All 696 tests are example-based.\n\nTargets for @given decorators:\n\n1. Handler round-trip invariant:\n   - handler.put(obj, path) → handler.get(path) === obj\n   - Test with random DataFrames (varying dtypes, shapes, NaN, empty)\n   - Test with random NumPy arrays (varying dtypes, shapes, edge values)\n   - Test with random Python objects via PickleHandler\n\n2. Cache key determinism:\n   - _create_cache_key(**kwargs) must be deterministic (same input → same key)\n   - Test with random kwargs dicts (strings, ints, floats, nested)\n   - Verify named params (prefix, description, custom_metadata) don't affect key\n\n3. Metadata backend contract:\n   - put_entry(key, data) → get_entry(key) returns equivalent data\n   - put_entry → delete_entry → get_entry returns None\n   - list_entries contains all put keys\n   - Test across JSON, SQLite, and sqlite_memory backends\n\n4. Serialization round-trips:\n   - compress_pickle: compress(data) → decompress() === data\n   - Test with random bytes, varying sizes, all codec combinations\n\nSubtasks:\n- [ ] Install hypothesis: uv add --dev hypothesis\n- [ ] Create tests/test_property_based.py\n- [ ] Handler round-trip tests with st.one_of for multiple types\n- [ ] Cache key determinism tests\n- [ ] Metadata backend contract tests (parametrized across backends)\n- [ ] Compression round-trip tests across codecs\n- [ ] Add to CI (hypothesis tests can be slow — use deadline setting)","design":"Create tests/test_property_based.py with @given decorators targeting handler round-trips, cache key determinism, metadata backend contracts, and compression round-trips. Use hypothesis strategies (st.dictionaries, st.floats, st.binary, st.text) to generate random inputs. Parametrize backend tests across json/sqlite/sqlite_memory. Set deadline=None for slow tests. Estimated effort: 2-3 days.","acceptance_criteria":"- hypothesis installed as dev dependency\n- At least 4 property-based test classes covering handlers, cache keys, backends, compression\n- All property tests pass with default hypothesis settings\n- No regressions in existing 696-test baseline","status":"closed","priority":1,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:05:07.3259734-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T14:53:06.8642469-05:00","closed_at":"2026-02-07T14:53:06.8642469-05:00","close_reason":"Closed","labels":["reliability","testing"],"dependencies":[{"issue_id":"CACHE-78r","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:05:07.333393-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-869","title":"Add hit/miss stats to @cached cache_info()","description":"decorators.py L231 — `_cache_info()` only returns static config (ttl, prefix, dir). No hit/miss counters. Users expect `cache_info()` to provide runtime statistics like Python's `functools.lru_cache().cache_info()`.","design":"1. Add instance variables `_hits: int = 0` and `_misses: int = 0` to `CacheDecorator`\n2. Increment `_hits` in `__call__` when cache returns a value, `_misses` when it falls through\n3. Add `_lock = threading.Lock()` for thread-safe counter updates\n4. Return `hits`, `misses`, `currsize` (via list_entries count or get_stats) in `_cache_info()` dict","acceptance_criteria":"- [ ] `cache_info()` returns `hits`, `misses`, `size` counts in addition to existing config\n- [ ] Hit/miss counters tracked per-decorated-function\n- [ ] Counters are thread-safe (use `threading.Lock` or atomic operations)\n- [ ] Test: After hits and misses, `cache_info()` returns correct counts\n- [ ] Test: Counters are per-function, not global\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":2,"issue_type":"feature","assignee":"GitHub Copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:01:16.9051304-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T12:53:08.2526344-05:00","closed_at":"2026-02-09T12:53:08.2526344-05:00","close_reason":"Feature complete: cache_info() now tracks hits, misses, and size. Added _hits and _misses counters with thread-safe updates. 4 comprehensive tests added. Merged to dev.","labels":["api","decorators","high"],"dependencies":[{"issue_id":"CACHE-869","depends_on_id":"CACHE-1kn","type":"related","created_at":"2026-02-08T13:03:08.2305016-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-8fu","title":"Cache Integrity Verification (fsck) Method","description":"Add a production-usable verify_integrity() / fsck() method to UnifiedCache that detects and optionally repairs inconsistencies between blob files and metadata. Inspired by TigerBeetle's StateChecker pattern — lightweight invariant validation that can run in production, not just tests.\n\nCurrent state: No integrity verification exists. Users have no way to detect orphaned blobs, dangling metadata entries, or size mismatches.\n\nInvariants to check:\n\n1. No orphaned blobs:\n   - Every file in cache_dir should have a corresponding metadata entry\n   - Files without metadata = orphaned (wasted disk space, invisible)\n\n2. No dangling metadata:\n   - Every metadata entry should point to an existing blob file\n   - Entries without files = dangling (get() will fail and auto-delete)\n\n3. Size consistency:\n   - metadata.file_size should match actual os.path.getsize()\n   - Mismatch suggests truncated write or external modification\n\n4. Hash consistency (optional, expensive):\n   - metadata.file_hash should match actual file hash\n   - Detects corruption or tampering\n\nAPI design:\n```python\ncache = cacheness()\n\n# Check only (no modifications)\nreport = cache.verify_integrity()\n# Returns: {\"orphaned_blobs\": [...], \"dangling_entries\": [...], \"size_mismatches\": [...]}\n\n# Check and repair\nreport = cache.verify_integrity(repair=True)\n# Deletes orphaned blobs, removes dangling entries\n```\n\nSubtasks:\n- [ ] Add verify_integrity(repair=False) method to UnifiedCache\n- [ ] Implement orphaned blob detection (scan cache_dir vs metadata keys)\n- [ ] Implement dangling metadata detection (metadata entries vs actual files)\n- [ ] Implement size mismatch detection\n- [ ] Optional: hash verification (gated by verify_hashes=True flag)\n- [ ] Add repair mode (clean up orphans and dangling entries)\n- [ ] Create tests/test_cache_integrity_verification.py\n- [ ] Document in API_REFERENCE.md","design":"Add verify_integrity() to UnifiedCache that scans both cache_dir (blob files) and metadata backend (entries) to find inconsistencies. Use iter_entry_summaries() for metadata scan (lightweight). Walk cache_dir for blob inventory. Compare sets to find orphans and dangles. Return structured dict report. Optional repair mode deletes orphans and removes dangling entries. Estimated effort: 1-2 days.","acceptance_criteria":"- verify_integrity() method on UnifiedCache returns structured report\n- Detects orphaned blobs (files with no metadata entry)\n- Detects dangling metadata (entries pointing to missing files)\n- Detects size mismatches between metadata and actual file\n- Optionally repairs (clean=True deletes orphans, removes dangling entries)\n- Tests cover all detected anomaly types\n- No regressions in existing 696-test baseline","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:05:47.4150238-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T16:23:20.5572394-05:00","closed_at":"2026-02-07T16:23:20.5572394-05:00","close_reason":"Implemented verify_integrity(repair, verify_hashes) method in core.py. Detects orphaned blobs, dangling metadata, size mismatches, hash mismatches. Optional repair mode. 21 tests in test_cache_integrity_verification.py — all passing.","labels":["reliability","testing"],"dependencies":[{"issue_id":"CACHE-8fu","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:05:47.4180242-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-8gw","title":"Add clear() alias for clear_all() on UnifiedCache","description":"API_REFERENCE.md L287 documents `clear()` but only `clear_all()` exists (core.py L1726). `clear()` is the natural/expected name matching `dict.clear()` and `functools.lru_cache` conventions. Should be added as an alias.","design":"Simple alias: `def clear(self): return self.clear_all()`","acceptance_criteria":"- [ ] `clear()` method added as alias for `clear_all()` on `UnifiedCache`\n- [ ] Behaves identically to `clear_all()`\n- [ ] Documented alongside `clear_all()`\n- [ ] Test: Verify `clear()` removes all entries\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:01:16.9498426-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T15:46:57.9137192-05:00","closed_at":"2026-02-08T15:46:57.9137192-05:00","close_reason":"Feature complete: clear() alias added as one-line wrapper for clear_all(). Tests pass. Merged to dev and pushed.","labels":["api","core","high"]}
{"id":"CACHE-8qx","title":"Export or remove undiscoverable decorator utilities","description":"`cache_function`, `memoize`, and `CacheContext` exist in decorators.py but are not in `__init__.py`'s `__all__`. They're not importable as part of the public API and not documented. Either export and document them, or remove them as dead code.","design":"1. Evaluate if `cache_function`, `memoize`, `CacheContext` add value over `@cached`\n2. If they're just aliases/wrappers, deprecate and remove\n3. If they serve distinct use cases, add to `__all__` and document\n4. Check if any tests depend on them","acceptance_criteria":"- [ ] `cache_function`, `memoize`, `CacheContext` either added to `__all__` or removed\n- [ ] If kept: documented in API_REFERENCE.md\n- [ ] If removed: deleted from decorators.py\n- [ ] Decision documented in commit message\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":4,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:58.131913-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:58.131913-05:00","labels":["cleanup","decorators","low"]}
{"id":"CACHE-8wi","title":"Implement @cache_async decorator for async functions","description":"Documented in API_REFERENCE.md L851 but no async code exists anywhere in `src/cacheness/`. Async/await is increasingly common in Python web frameworks (FastAPI, Starlette). Users caching async API calls currently have no decorator support.\n\nNo async code exists in the codebase — `core.py` is entirely synchronous with `threading.RLock()`.","design":"1. Create `cache_async` class in decorators.py (or extend `cached`)\n2. Use `asyncio.to_thread()` to wrap sync cache operations (put/get) for non-blocking async\n3. Wrapper function should be `async def` that `await`s cache checks\n4. Attach same management methods (cache_clear, cache_info, cache_key)\n5. Consider: native async backends are out of scope — just wrap sync ops in thread pool","acceptance_criteria":"- [ ] `@cache_async` decorator works with `async def` functions\n- [ ] Supports same parameters as `@cached` (ttl, prefix, ignore_errors, etc.)\n- [ ] Cache operations run in thread pool to avoid blocking event loop\n- [ ] `cache_clear()`, `cache_info()`, `cache_key()` methods attached to wrapper\n- [ ] Test: Async function is cached correctly\n- [ ] Test: Concurrent async calls share cache\n- [ ] Test: TTL expiration works with async\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":3,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.0448772-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:17.0448772-05:00","labels":["async","decorators","medium"],"dependencies":[{"issue_id":"CACHE-8wi","depends_on_id":"CACHE-869","type":"related","created_at":"2026-02-08T13:03:08.2939206-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-9bd","title":"Make query_meta() work across all backends","description":"core.py L498 — `query_meta()` only works with SQLite (uses raw `JSON_EXTRACT` SQL). For JSON and Postgres backends, it silently returns empty results or `None`. The method should work consistently across all backends, falling back to Python-side filtering when SQL JSON queries aren't available.","design":"1. In `query_meta()`, keep the SQLite fast path (JSON_EXTRACT)\n2. For non-SQLite backends, fall back to iterating `iter_entry_summaries()` with Python-side dict matching\n3. Ensure consistent return type (list of cache_keys) across all paths\n4. Consider adding PostgreSQL JSON query support (`-\u003e\u003e`  operator) as optimization","acceptance_criteria":"- [ ] `query_meta()` works with JsonBackend (Python-side filtering fallback)\n- [ ] `query_meta()` works with PostgresBackend (JSON query or Python fallback)\n- [ ] Consistent return type across all backends\n- [ ] Test: `query_meta()` returns correct results with JsonBackend\n- [ ] Test: `query_meta()` returns correct results with SqliteBackend (existing)\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":3,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.0760395-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T15:19:45.5577569-05:00","closed_at":"2026-02-12T15:19:45.5577569-05:00","close_reason":"Implemented: query_meta() now works across all backends via Python-side fallback. SQLite keeps JSON_EXTRACT fast path. 895 tests pass.","labels":["core","medium","metadata"]}
{"id":"CACHE-a6a","title":"Fix _lock Thread Safety + Concurrency Stress Tests","description":"Fix the thread safety gap in UnifiedCache and add proper concurrency stress tests. Currently self._lock is created at __init__ (line 87 of core.py) but NEVER acquired anywhere — making all UnifiedCache methods completely unsynchronized at the coordination layer.\n\nThe existing concurrency tests in test_sqlite_concurrency.py use unique keys per thread, which avoids the actual race condition. They also accept \"database is locked\" errors as OK.\n\nPart 1 — Fix the lock:\n- Acquire self._lock in put(), get(), update_data(), invalidate(), delete_where(), touch(), and other methods that perform multi-step blob+metadata operations\n- The lock protects the blob-write → metadata-write sequence from interleaving\n- Backend-level locks (JSON Lock, SQLite RLock) still protect individual backend operations\n- Use context manager: with self._lock: for clean acquisition/release\n\nPart 2 — Add same-key concurrency tests:\n- N threads all calling put() with the SAME cache key simultaneously\n- Verify: exactly 1 entry exists after all threads complete\n- Verify: no orphaned blobs\n- Verify: the stored data matches one of the thread's inputs\n\nPart 3 — Add cross-operation concurrency tests:\n- Threads mixing put(), get(), invalidate() on overlapping keys\n- Use threading.Barrier to maximize collision probability\n- Test across JSON, SQLite, and sqlite_memory backends (current tests only cover SQLite)\n\nPart 4 — Tighten existing concurrency tests:\n- Remove acceptance of \"database is locked\" as OK — these should be retried or prevented\n- Add proper assertions instead of filtering out errors\n\nSubtasks:\n- [ ] Acquire self._lock in put() around blob-write + metadata-write sequence\n- [ ] Acquire self._lock in get() around metadata-read + blob-read sequence\n- [ ] Acquire self._lock in update_data(), invalidate(), touch()\n- [ ] Add same-key concurrent put() test\n- [ ] Add put()/get()/invalidate() interleaving test\n- [ ] Add JSON backend concurrency test\n- [ ] Add sqlite_memory concurrency test\n- [ ] Tighten existing test_sqlite_concurrency.py assertions\n- [ ] Verify no deadlocks with backend-level locks (JSON, SQLite)","design":"Fix is straightforward: wrap multi-step operations in with self._lock:. The lock scope should be the entire put() or get() call — not just the metadata portion — because we need atomicity of the blob+metadata pair. Tests use threading.Barrier to synchronize thread start for maximum collision probability. Parametrize across backends. Estimated effort: 1-2 days.","acceptance_criteria":"- self._lock is acquired in put(), get(), update_data(), invalidate()\n- Same-key concurrent put() test passes (N threads, 1 key, no corruption)\n- Concurrent put()/get() interleaving test passes\n- Concurrent tests added for JSON and sqlite_memory backends (not just SQLite)\n- Existing concurrency tests still pass\n- No regressions in existing 696-test baseline","status":"closed","priority":2,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:06:09.5968365-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T16:23:20.4204367-05:00","closed_at":"2026-02-07T16:23:20.4204367-05:00","close_reason":"Fixed _lock: changed threading.Lock() to RLock(), wrapped all public mutation/read methods (put, get, get_metadata, update_data, touch, invalidate, delete_where, delete_matching, get_batch, delete_batch, touch_batch, clear_all, verify_integrity) with 'with self._lock:'. Created test_concurrency_stress.py with 16 stress tests — all passing.","labels":["bug","concurrency","reliability","testing"],"dependencies":[{"issue_id":"CACHE-a6a","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:06:09.6000521-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-a7a","title":"Implement cleanup_by_size() on all metadata backends","description":"core.py L1548 calls `self.metadata_backend.cleanup_by_size(target_size)` in `_enforce_size_limit()`, but NO metadata backend implements this method. When cache exceeds `max_cache_size_mb`, `_enforce_size_limit()` crashes with `AttributeError`.\n\nThe method is called after every `put()` operation (L906) when `max_cache_size_mb` is configured. This is a ticking time bomb for any user who sets a cache size limit.\n\n**Root cause:** `cleanup_by_size()` was referenced in `_enforce_size_limit()` but never implemented on the abstract base or any concrete backend.","design":"1. Add `cleanup_by_size(target_size_mb: float) -\u003e int` to `MetadataBackend` abstract base (metadata.py L176)\n2. Implement LRU eviction on each backend:\n   - **JsonBackend**: Sort entries by `last_accessed`, delete oldest until total_size_mb \u003c= target\n   - **SqliteBackend**: `DELETE FROM ... ORDER BY last_accessed ASC LIMIT n` with size check\n   - **PostgresBackend**: Same SQL approach with PostgreSQL syntax\n3. Each implementation should also delete the associated blob files (delegate to `invalidate()` pattern)\n4. `_enforce_size_limit()` in core.py should handle the case where the backend doesn't support it gracefully (hasattr check)","acceptance_criteria":"- [ ] `cleanup_by_size()` added to `MetadataBackend` abstract base class\n- [ ] Implemented on `JsonBackend` with LRU eviction\n- [ ] Implemented on `SqliteBackend` with SQL-based LRU eviction\n- [ ] Implemented on `PostgresBackend` with SQL-based LRU eviction\n- [ ] Blob files are deleted alongside metadata during eviction\n- [ ] `_enforce_size_limit()` gracefully handles backends without the method\n- [ ] Test: Set `max_cache_size_mb=0.001`, put entries until eviction triggers, verify oldest entries removed\n- [ ] Test: Verify blob files are cleaned up during eviction\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":1,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:00:28.8995223-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T14:55:38.3671814-05:00","closed_at":"2026-02-08T14:55:38.3671814-05:00","close_reason":"Fixed: Implemented cleanup_by_size() on all metadata backends with LRU eviction. Backend parity test passes. One integration test marked as skipped due to hanging (tracked in CACHE-mdd for investigation). All other tests passing (787 passed, 71 skipped).","labels":["critical","metadata","runtime-crash"]}
{"id":"CACHE-adi","title":"Multi-tenant isolation: namespace support + schema versioning","description":"When multiple Cacheness instances share the same PostgreSQL server and/or S3 bucket, there is zero isolation. Critical conflicts: cache key collisions, signing key mismatch causing destructive auto-deletes, TTL/eviction cross-fire, nuclear clear_all(), global stats, cleanup_on_init races, S3 blob orphaning.\n\nApproach: TABLE-PER-NAMESPACE with a namespace registry table. Each namespace gets its own set of tables (cache_entries_{id}, cache_stats_{id}), providing strong isolation, independent maintenance (VACUUM/DROP per namespace), and clean project removal. A cacheness_namespaces registry table tracks all namespaces with per-namespace schema versioning.\n\nSigning model: each namespace has its own signing key, user-provided at runtime (never stored). Registry entries signed with their namespace's key  verified on connect for fast fail. Different namespaces can have different keys.\n\nNamespace ID: ^[a-z0-9_]{1,48}$  safe for SQL identifiers. Backward compat: namespace_id='default' uses existing unsuffixed tables (cache_entries, cache_stats)  zero migration for existing databases.\n\nAll three backends (PostgreSQL, SQLite, JSON) get namespace support and schema versioning.","status":"open","priority":2,"issue_type":"epic","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:13:37.8814582-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:31:58.0426339-05:00","labels":["architecture","postgresql","s3","security","sqlite"]}
{"id":"CACHE-adi.1","title":"Schema versioning: MetadataBackend base class interface","description":"Add schema versioning infrastructure to the MetadataBackend abstract base class. Add _run_migrations() method, schema_version property, and _get_schema_version()/_set_schema_version() internal methods. Define migration as a (from_version, to_version, callable) tuple. Migrations run sequentially on init.\n\nAdd namespace registry abstract interface: create_namespace(id), drop_namespace(id), list_namespaces(), get_namespace_tables(id). Namespace ID validation: ^[a-z0-9_]{1,48}$ enforced at creation. Default namespace 'default' maps to existing unsuffixed tables for backward compatibility.\n\nv1 = current schema (no namespace registry). v2 migration = create registry + register 'default' namespace pointing to existing tables.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:14:25.5243443-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T09:45:36.4462566-05:00","closed_at":"2026-02-13T09:45:36.4462566-05:00","close_reason":"Implemented: schema versioning + namespace registry interface on MetadataBackend ABC","labels":["architecture","backend"],"dependencies":[{"issue_id":"CACHE-adi.1","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:14:25.5259538-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.10","title":"Scope eviction and cleanup to namespace","description":"Ensure all eviction strategies (LRU, LFU, FIFO, TTL, size-based) operate on the namespace's own tables only  inherently isolated since each namespace has separate tables. cleanup_on_init only touches the active namespace's tables. clear_all() drops and recreates the active namespace's tables (add clear_all_namespaces() for nuclear option that iterates registry). max_entries/max_size_bytes are per-namespace limits. Per-namespace stats row in each namespace's cache_stats table.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:31.6139693-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:27:23.7967356-05:00","labels":["architecture","backend"],"dependencies":[{"issue_id":"CACHE-adi.10","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:31.615527-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.10","depends_on_id":"CACHE-adi.6","type":"blocks","created_at":"2026-02-12T17:16:32.1207284-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.10","depends_on_id":"CACHE-adi.7","type":"blocks","created_at":"2026-02-12T17:16:32.5009599-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.10","depends_on_id":"CACHE-adi.8","type":"blocks","created_at":"2026-02-12T17:16:32.8889146-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.11","title":"Signing key coordination documentation","description":"Document signing key behavior in table-per-namespace setup. Key model: each namespace has its own signing key, provided by user at runtime (never stored in any registry or backend). Different namespaces can have different keys.\n\nRegistry entry for namespace X is signed with X's key  verified on connect (fast fail if wrong key). All cache entries in cache_entries_{id} signed with that namespace's key.\n\nDocument: how signing keys work per-namespace, how to configure different keys per namespace, interaction with delete_invalid_signatures flag, migration guide for existing users (existing entries stay signed with existing key under 'default' namespace). Update docs/SECURITY.md and docs/CONFIGURATION.md.","status":"open","priority":1,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:37.9044325-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:31:42.4075768-05:00","labels":["docs","security"],"dependencies":[{"issue_id":"CACHE-adi.11","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:37.9054692-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.11","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-12T17:16:33.2723226-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.12","title":"Multi-tenant integration tests","description":"Comprehensive integration tests for table-per-namespace isolation. Test scenarios: (1) Two namespaces on same PostgreSQL, verify zero cross-talk for get/put/delete/list/stats/eviction via separate tables. (2) Two namespaces on same SQLite file with separate table sets. (3) Two namespaces on same JSON cache_dir with separate files. (4) S3 prefix isolation per namespace. (5) cache-mode + storage-mode instances on same backend with different namespaces. (6) Signing verification across namespaces with shared key. (7) Migration from v1 (no registry) to v2  existing data preserved in 'default' namespace using unsuffixed tables. (8) create_namespace/drop_namespace lifecycle  verify tables created/dropped. (9) Namespace ID validation rejects invalid identifiers.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:44.5361517-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:27:32.2579539-05:00","labels":["testing"],"dependencies":[{"issue_id":"CACHE-adi.12","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:44.5371908-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.12","depends_on_id":"CACHE-adi.10","type":"blocks","created_at":"2026-02-12T17:16:33.6579352-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.12","depends_on_id":"CACHE-adi.11","type":"blocks","created_at":"2026-02-12T17:16:34.045772-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.13","title":"Namespace plumbing: UnifiedCache -\u003e factory -\u003e backend init","description":"Wire config.namespace through create_metadata_backend() to backend constructors. Each backend __init__ accepts namespace param, stores it, uses it to determine which tables/files to operate on. Hard prereq for CACHE-adi.6/7/8.","status":"in_progress","priority":1,"issue_type":"task","assignee":"radioflyer28","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-13T11:26:27.7176412-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T11:27:33.616297-05:00","labels":["blocker","phase-2"],"dependencies":[{"issue_id":"CACHE-adi.13","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-13T11:26:27.7197583-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.13","depends_on_id":"CACHE-adi","type":"blocks","created_at":"2026-02-13T11:26:27.7212897-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.14","title":"Filesystem blob namespace isolation","description":"Add namespace-scoped subdirectories to LocalBlobBackend. Pattern: cache_dir/{namespace_id}/ for non-default namespaces. Default namespace uses existing paths for backward compat. Without this, two namespaces sharing same cache_dir collide on blob filenames. CACHE-adi.9 covers S3 only; this covers local filesystem.","status":"open","priority":1,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-13T11:26:37.6385061-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T11:26:37.6385061-05:00","labels":["blocker","phase-2"],"dependencies":[{"issue_id":"CACHE-adi.14","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-13T11:26:37.6406144-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.14","depends_on_id":"CACHE-adi","type":"blocks","created_at":"2026-02-13T11:26:37.6421631-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.15","title":"Per-namespace migration runner","description":"Currently run_migrations(DEFAULT_NAMESPACE) is called once in __init__ - non-default namespaces never get migrated. Add startup or lazy-access logic that iterates the namespace registry and runs run_migrations() on each. Depends on backends knowing their active namespace.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-13T11:26:45.8782897-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T11:26:45.8782897-05:00","labels":["correctness","phase-2"],"dependencies":[{"issue_id":"CACHE-adi.15","depends_on_id":"CACHE-adi.13","type":"blocks","created_at":"2026-02-13T11:26:45.8813219-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.15","depends_on_id":"CACHE-adi","type":"blocks","created_at":"2026-02-13T11:26:45.8833966-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.16","title":"Per-namespace signing key support","description":"CacheEntrySigner holds a single key for the entire instance - needs per-namespace key resolution. Namespace registry stores key hint/identifier, signer accepts a key resolver callback or dict. Implementation counterpart to CACHE-adi.11 (docs-only).","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-13T11:26:53.1435727-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T11:26:53.1435727-05:00","labels":["correctness","phase-2"],"dependencies":[{"issue_id":"CACHE-adi.16","depends_on_id":"CACHE-adi.13","type":"blocks","created_at":"2026-02-13T11:26:53.1456809-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.16","depends_on_id":"CACHE-adi","type":"blocks","created_at":"2026-02-13T11:26:53.1472539-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.2","title":"Schema versioning: PostgreSQL implementation","description":"Implement schema versioning for PostgreSQL backend. Create cacheness_namespaces registry table (namespace_id TEXT PK, display_name TEXT, schema_version INT DEFAULT 1, created_at TIMESTAMP). On init: CREATE TABLE IF NOT EXISTS cacheness_namespaces. Check/run migrations per namespace.\n\nv1-\u003ev2 migration: create cacheness_namespaces table, INSERT ('default', 'Default', 1) pointing to existing cache_entries/cache_stats tables. Dynamic table creation via SQLAlchemy Table() objects (not ORM mapped classes)  factory function get_entries_table(namespace_id) returns Table with name cache_entries_{id}. Uses SQLAlchemy for DDL.","status":"closed","priority":2,"issue_type":"task","assignee":"radioflyer28","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:14:33.5191516-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T10:46:26.7587177-05:00","closed_at":"2026-02-13T10:46:26.7587177-05:00","close_reason":"Implemented PG schema versioning + namespace registry. 26 tests (skipped without live PG). 1000 total passing.","labels":["backend","postgresql"],"dependencies":[{"issue_id":"CACHE-adi.2","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:14:33.520173-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.2","depends_on_id":"CACHE-adi.1","type":"blocks","created_at":"2026-02-12T17:16:12.7673747-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.3","title":"Schema versioning: SQLite implementation","description":"Implement schema versioning for SQLite backend. Create cacheness_namespaces registry table (namespace_id TEXT PK, display_name TEXT, schema_version INT DEFAULT 1, created_at TEXT). On init: CREATE TABLE IF NOT EXISTS. Check/run migrations per namespace.\n\nv1-\u003ev2 migration: create cacheness_namespaces table, INSERT ('default', 'Default', 1) pointing to existing cache_entries/cache_stats tables. Dynamic table creation for new namespaces: cache_entries_{id}, cache_stats_{id}. Uses raw SQL via sqlite3.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:14:40.570865-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T09:57:15.8062962-05:00","closed_at":"2026-02-13T09:57:15.8062962-05:00","close_reason":"Implemented: SQLite schema versioning + namespace registry with CacheNamespace ORM model","labels":["backend","sqlite"],"dependencies":[{"issue_id":"CACHE-adi.3","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:14:40.5724499-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.3","depends_on_id":"CACHE-adi.1","type":"blocks","created_at":"2026-02-12T17:16:17.6123937-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.4","title":"Schema versioning: JSON implementation","description":"Implement schema versioning for JSON backend. Create _namespaces.json registry file: dict of {namespace_id: {display_name, schema_version, created_at}}. On load: check file exists, read registry, run per-namespace migrations.\n\nv1-\u003ev2 migration: create _namespaces.json with 'default' entry pointing to existing metadata.json. New namespaces get separate files: {namespace_id}_metadata.json. Default namespace uses existing unsuffixed metadata.json for backward compat.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:14:48.6763295-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T10:25:03.0009974-05:00","closed_at":"2026-02-13T10:25:03.0009974-05:00","close_reason":"Implemented JSON backend schema versioning + namespace registry. 33 tests, 1000 total passing.","labels":["backend","json"],"dependencies":[{"issue_id":"CACHE-adi.4","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:14:48.6773685-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.4","depends_on_id":"CACHE-adi.1","type":"blocks","created_at":"2026-02-12T17:16:18.0037604-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.5","title":"Add namespace to CacheConfig and MetadataBackend interface","description":"Add namespace: str = 'default' to CacheConfig. Add namespace parameter to MetadataBackend __init__. Backend resolves namespace to table names via registry: 'default' -\u003e unsuffixed tables, other -\u003e cache_entries_{id}/cache_stats_{id}.\n\nSigning model: each namespace has its own signing key, user-provided at runtime (never stored). Signing key is per-namespace, passed via SecurityConfig. Registry entry for namespace X is signed with X's key  verified on connect for fast fail. CacheEntrySigner scoped to the active namespace's key.\n\ncreate_namespace(id, display_name=None): validate id (^[a-z0-9_]{1,48}$), INSERT into registry, sign registry entry with provided key, CREATE per-namespace tables. drop_namespace(id): verify registry signature, DROP tables, DELETE from registry. list_namespaces(): SELECT from registry (unsigned listing is fine for discovery). Namespace is set at init and immutable.","status":"closed","priority":2,"issue_type":"task","assignee":"radioflyer28","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:14:57.97208-05:00","created_by":"radioflyer28","updated_at":"2026-02-13T11:05:58.4174271-05:00","closed_at":"2026-02-13T11:05:58.4174271-05:00","close_reason":"Namespace param added to CacheConfig/UnifiedCache. 14 tests. 1014 passed, 97 skipped. Merged to dev.","labels":["architecture","backend"],"dependencies":[{"issue_id":"CACHE-adi.5","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:14:57.9742506-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.5","depends_on_id":"CACHE-adi.1","type":"blocks","created_at":"2026-02-12T17:16:18.3883899-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.6","title":"Namespace: PostgreSQL implementation","description":"Implement table-per-namespace for PostgreSQL. Dynamic Table() objects via factory: get_entries_table(namespace_id) and get_stats_table(namespace_id). 'default' namespace returns existing cache_entries/cache_stats tables. Other namespaces return cache_entries_{id}/cache_stats_{id}.\n\ncreate_namespace(): validates id, INSERTs into cacheness_namespaces, runs CREATE TABLE for entries + stats tables with proper schema. drop_namespace(): DROP TABLE CASCADE + DELETE from registry. All queries use dynamically-resolved table objects. Remove dead table_prefix code. Schema migration runs per-namespace by iterating registry.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:05.2723803-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:26:55.0735929-05:00","labels":["backend","postgresql"],"dependencies":[{"issue_id":"CACHE-adi.6","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:05.2739698-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.6","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-12T17:16:24.4963752-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.6","depends_on_id":"CACHE-adi.2","type":"blocks","created_at":"2026-02-12T17:16:24.9024236-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.7","title":"Namespace: SQLite implementation","description":"Implement table-per-namespace for SQLite. Dynamic table name resolution: 'default' -\u003e existing cache_entries/cache_stats, other -\u003e cache_entries_{id}/cache_stats_{id}.\n\ncreate_namespace(): validate id, INSERT into cacheness_namespaces, CREATE TABLE for entries + stats with proper schema. drop_namespace(): DROP TABLE + DELETE from registry. All SQL queries use f-string table names (safe because namespace_id is validated ^[a-z0-9_]{1,48}$). Schema migration iterates registry and ALTERs each namespace's tables independently.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:09.9483787-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:27:00.9636485-05:00","labels":["backend","sqlite"],"dependencies":[{"issue_id":"CACHE-adi.7","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:09.9499293-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.7","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-12T17:16:25.4080707-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.7","depends_on_id":"CACHE-adi.3","type":"blocks","created_at":"2026-02-12T17:16:25.7840045-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.8","title":"Namespace: JSON implementation","description":"Implement per-namespace file isolation for JSON backend. 'default' namespace uses existing metadata.json. Other namespaces use {namespace_id}_metadata.json.\n\ncreate_namespace(): validate id, add to _namespaces.json registry, create empty {id}_metadata.json. drop_namespace(): delete {id}_metadata.json + remove from registry. All read/write operations resolve to the namespace's dedicated file. No cross-namespace data possible since files are physically separate.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:15.4090721-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:27:06.8190592-05:00","labels":["backend","json"],"dependencies":[{"issue_id":"CACHE-adi.8","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:15.4101129-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.8","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-12T17:16:26.1608479-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.8","depends_on_id":"CACHE-adi.4","type":"blocks","created_at":"2026-02-12T17:16:26.6597319-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-adi.9","title":"Namespace: S3 prefix auto-derivation","description":"When namespace is set, auto-derive S3 blob prefix as {base_prefix}/{namespace_id}/. Ensures blob-level isolation matches metadata table isolation. Backward compatible: namespace_id='default' uses legacy prefix (no extra path segment). drop_namespace() should also clean up S3 blobs under the namespace prefix. Document S3 key structure in docs.","status":"open","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T17:15:21.6810153-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T17:27:13.7021941-05:00","labels":["backend","s3"],"dependencies":[{"issue_id":"CACHE-adi.9","depends_on_id":"CACHE-adi","type":"parent-child","created_at":"2026-02-12T17:15:21.6820434-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-adi.9","depends_on_id":"CACHE-adi.5","type":"blocks","created_at":"2026-02-12T17:16:27.0406135-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-blob-cleanup","title":"Fix invalidate/delete to clean up blob files","description":"invalidate(), delete_where(), and delete_matching() only remove metadata entries but leave blob files on disk, creating orphans. Users must currently run verify_integrity(repair=True) to reclaim disk space.\\n\\nThe fix: before removing each metadata entry, read its actual_path from metadata and delete the blob file. This should be done in each method that removes entries.","acceptance_criteria":"- invalidate() deletes the blob file before removing metadata\\n- delete_where() deletes blob files for all matched entries\\n- delete_matching() deletes blob files for all matched entries\\n- Tests verify no orphaned blobs remain after each operation\\n- verify_integrity() reports 0 issues after invalidate/delete operations","status":"closed","priority":1,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T16:58:08.0444253-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T17:19:45.5325614-05:00","closed_at":"2026-02-07T17:19:45.5325614-05:00","close_reason":"invalidate() now deletes blob files before removing metadata. 10 tests added. All delete operations benefit via delegation.","labels":["bug","core"]}
{"id":"CACHE-c5p","title":"Unify MetadataBackend ABC into single canonical interface","description":"Reconcile two MetadataBackend ABCs: metadata.py (L180) and storage/backends/base.py (L12). Make storage/backends/base.py the canonical ABC. Add missing methods: cleanup_by_size(), iter_entry_summaries(). Fix remove_entry() return type to bool. Make metadata.py re-export/alias the canonical ABC. Update all concrete implementations (JsonBackend, SqliteBackend, PostgresBackend in metadata.py; JsonBackend, SqliteBackend in storage/backends/) to use the unified ABC.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-11T16:56:02.1267504-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T17:24:44.4208306-05:00","closed_at":"2026-02-11T17:24:44.4208306-05:00","close_reason":"Unified MetadataBackend ABC - merged to dev","labels":["phase-2","refactor","storage-mode"],"dependencies":[{"issue_id":"CACHE-c5p","depends_on_id":"CACHE-i4b","type":"blocks","created_at":"2026-02-11T16:56:02.1519092-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-c5p","depends_on_id":"CACHE-i4b","type":"parent-child","created_at":"2026-02-11T16:56:13.2410801-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-d0z","title":"Implement set_default_cache() global function","description":"Documented in API_REFERENCE.md L1035 but not implemented. Currently each `@cached` decorator creates its own `UnifiedCache` instance with separate metadata backend, cache dir, etc. A global default would let users configure once and reuse everywhere.\n\nWithout this, multiple decorators in the same app create multiple independent cache databases.","design":"1. Module-level `_default_cache: Optional[UnifiedCache] = None`\n2. `set_default_cache(cache)` sets it, `get_default_cache()` retrieves it\n3. In `CacheDecorator.__call__`, check for default cache if no instance provided\n4. Export both functions from `__init__.py`","acceptance_criteria":"- [ ] `set_default_cache(cache_instance)` function in `__init__.py`\n- [ ] `get_default_cache() -\u003e Optional[UnifiedCache]` function\n- [ ] `@cached` decorator uses default cache when no instance provided\n- [ ] Thread-safe (module-level lock)\n- [ ] Test: Set default, decorator uses it\n- [ ] Test: Explicit cache instance overrides default\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":4,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:58.1207757-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:58.1207757-05:00","labels":["api","core","low"]}
{"id":"CACHE-delete-dev-planning","title":"Delete stale DEVELOPMENT_PLANNING.md","description":"docs/DEVELOPMENT_PLANNING.md has ~200 unchecked checkboxes, most of which are either completed or now tracked in beads-mcp. The doc is misleading about project status.\\n\\nNow that beads-mcp is the source of truth for issue tracking and .github/copilot-instructions.md covers architecture context, this file is redundant. Delete it and remove any references to it.","acceptance_criteria":"- docs/DEVELOPMENT_PLANNING.md is deleted\\n- .github/copilot-instructions.md no longer references DEVELOPMENT_PLANNING.md\\n- No broken doc links remain","status":"closed","priority":3,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T16:58:08.0491658-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T17:19:45.5207446-05:00","closed_at":"2026-02-07T17:19:45.5207446-05:00","close_reason":"File deleted, all 6 references cleaned up.","labels":["chore","docs"]}
{"id":"CACHE-fi4","title":"Phase 2 docs update and dead code cleanup","description":"Update ARCHITECTURE.md (unified ABC, composition model), BLOB_STORE.md (enhanced feature set), CONFIGURATION.md (storage_mode changes), API_REFERENCE.md. Remove dead code from refactoring. Update copilot-instructions.md baselines.","status":"closed","priority":3,"issue_type":"task","assignee":"copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-11T16:56:02.1941006-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T12:10:34.1588288-05:00","closed_at":"2026-02-12T12:10:34.1588288-05:00","close_reason":"Phase 2 docs updated: ARCHITECTURE.md (composition model, unified ABC), BLOB_STORE.md (enhanced features), CONFIGURATION.md (passthrough architecture), API_REFERENCE.md (verify_integrity, composition note). Merged to dev at b635689.","labels":["docs","phase-2","storage-mode"],"dependencies":[{"issue_id":"CACHE-fi4","depends_on_id":"CACHE-2sf","type":"blocks","created_at":"2026-02-11T16:56:13.1938441-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-fi4","depends_on_id":"CACHE-i4b","type":"parent-child","created_at":"2026-02-11T16:56:13.2512623-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-fpg","title":"Redesign cache key API with on parameter to prevent namespace collisions","description":"The current `put()` and `get()` signatures have problematic parameter handling that can lead to namespace collisions between user's cache key parameters and cache control parameters.\n\n**Current Problem:**\n```python\ndef put(self, data, prefix=\"\", description=\"\", custom_metadata=None, **kwargs)\ndef get(self, cache_key=None, ttl_seconds=None, prefix=\"\", **kwargs)\n```\n\nCache control parameters (`prefix`, `description`, `custom_metadata`) share namespace with user's key parameters. Example collision:\n```python\n# User naturally has 'description' as a key parameter\ncache.put(api_docs, endpoint=\"/users\", description=\"Get all users\")\n# ❌ 'description' consumed as metadata, NOT part of cache key!\n```\n\n**Solution: Add `on` parameter for explicit key parameters**\n\nAll cache key methods get consistent signature:\n- `cache_key` parameter for explicit key (highest priority)\n- `on` parameter for dict-based key params (no collision possible)\n- `**kwargs` for legacy compatibility (lowest priority)\n\nPriority: `cache_key` → `on` → `**kwargs`\n\n**Methods to update:**\n- `put()`, `get()`, `invalidate()`, `update_data()`, `touch()`, `get_metadata()`\n- Future methods: `exists()`, `get_with_metadata()`, `time_to_live()`","design":"**New Signatures:**\n```python\ndef put(self, data, cache_key=None, on=None, prefix=\"\", description=\"\", \n        custom_metadata=None, ttl_seconds=None, **kwargs)\n\ndef get(self, cache_key=None, on=None, ttl_seconds=None, prefix=\"\", **kwargs)\n\ndef invalidate(self, cache_key=None, on=None, prefix=\"\", **kwargs)\n\ndef update_data(self, data, cache_key=None, on=None, **kwargs)\n\ndef touch(self, cache_key=None, on=None, **kwargs)\n\ndef get_metadata(self, cache_key=None, on=None, **kwargs)\n```\n\n**Resolution Logic (add to each method):**\n```python\nif cache_key is None:\n    if on is not None:\n        cache_key = self._create_cache_key(on)\n    else:\n        cache_key = self._create_cache_key(kwargs)\n```\n\n**Three usage patterns:**\n```python\n# Pattern 1: Explicit cache key (fastest, clearest)\ncache.put(data, cache_key=\"my-key-123\", prefix=\"logs\")\ncache.get(cache_key=\"my-key-123\")\n\n# Pattern 2: Dict-based (new, no collision)\ncache.put(data, on={'date': '2026-02-08', 'description': 'user value'})\ncache.get(on={'date': '2026-02-08', 'description': 'user value'})\n\n# Pattern 3: kwargs-based (legacy)\ncache.put(data, date='2026-02-08', region='CA')\ncache.get(date='2026-02-08', region='CA')\n```","acceptance_criteria":"- [ ] `cache_key` and `on` parameters added to: `put()`, `get()`, `invalidate()`, `update_data()`, `touch()`, `get_metadata()`\n- [ ] Resolution priority implemented: `cache_key` → `on` → `**kwargs`\n- [ ] Backward compatibility: existing kwargs usage still works\n- [ ] No namespace collision with new patterns\n- [ ] Test: `description='x'` can be both key param (via `on`) and metadata\n- [ ] Test: Explicit `cache_key` works in all methods\n- [ ] Test: `on` parameter works in all methods\n- [ ] Test: kwargs fallback still works (legacy)\n- [ ] Test: Priority order verified (cache_key overrides on, on overrides kwargs)\n- [ ] Documentation updated in docstrings\n- [ ] API_REFERENCE.md updated with new patterns\n- [ ] Examples updated to show recommended usage\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":1,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:24:26.0700987-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T12:19:06.0049435-05:00","closed_at":"2026-02-11T12:19:06.0049435-05:00","close_reason":"Implemented: added on parameter to all 9 cache key methods, cache_key to put(), _resolve_cache_key() helper, updated decorators to use cache_key= directly. 818 passed, 64 skipped.","labels":["api","breaking-change","medium","storage-mode-prereq"],"comments":[{"id":1,"issue_id":"CACHE-fpg","author":"radioflyer28","text":"Reopened","created_at":"2026-02-08T18:27:29Z"}]}
{"id":"CACHE-hv8","title":"Add put_batch() operation for bulk ingestion","description":"`get_batch()`, `delete_batch()`, and `touch_batch()` all exist, but `put_batch()` does not. This is an API symmetry gap — users doing bulk ingestion must loop over `put()` calls individually.","design":"Follow the pattern of other batch operations. Take a list of items, iterate with shared lock, delegate to `put()` internals. Consider optimizing metadata writes (e.g., batch SQLite inserts).","acceptance_criteria":"- [ ] `put_batch(items: List[Tuple[str, Any, dict]]) -\u003e int` added to `UnifiedCache`\n- [ ] Accepts list of `(cache_key_or_kwargs, value, metadata)` tuples\n- [ ] Returns count of successfully stored entries\n- [ ] Optimized for batch operations (shared lock, batch metadata writes where possible)\n- [ ] Test: Store 10 entries via `put_batch()`, verify all retrievable\n- [ ] Test: Partial failure handling (some entries fail, others succeed)\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":3,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.0330925-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:17.0330925-05:00","labels":["api","core","medium"]}
{"id":"CACHE-hxh","title":"Add time_to_live() remaining TTL query","description":"No way to ask \"how long until this entry expires?\" Users can get `created_at` from metadata and know the TTL config, but computing remaining TTL manually is cumbersome. A convenience method would improve the API for time-sensitive caching use cases.","design":"```python\ndef time_to_live(self, cache_key: str) -\u003e Optional[float]:\n    with self._lock:\n        entry = self.metadata_backend.get_entry(cache_key)\n        if entry is None:\n            return None\n        ttl = entry.get('ttl_seconds') or self.config.storage.default_ttl_seconds\n        if not ttl:\n            return None\n        created = entry.get('created_at', 0)\n        remaining = ttl - (time.time() - created)\n        return max(0.0, remaining)\n```","acceptance_criteria":"- [ ] `time_to_live(cache_key) -\u003e Optional[float]` returns remaining seconds, or None if no TTL\n- [ ] Returns negative value for expired entries (or 0)\n- [ ] Returns None if entry not found\n- [ ] Thread-safe\n- [ ] Test: Entry with TTL returns correct remaining time\n- [ ] Test: Entry without TTL returns None\n- [ ] Test: Expired entry returns 0 or negative\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":3,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:17.1128351-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:17.1128351-05:00","labels":["api","core","medium"]}
{"id":"CACHE-i4b","title":"Storage Mode - Phase 2: Refactor UnifiedCache to layer on BlobStore","description":"Reconcile two MetadataBackend ABCs. Move xxhash key generation, integrity, signing, custom metadata into BlobStore. Refactor UnifiedCache to compose BlobStore + cache concerns (TTL, eviction, stats). storage_mode becomes 'use BlobStore directly without cache layer'. Consider extracting storage as separate package.","status":"closed","priority":2,"issue_type":"epic","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:16:43.9638239-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T12:10:47.5619589-05:00","closed_at":"2026-02-12T12:10:47.5619589-05:00","close_reason":"All Phase 2 sub-tasks completed: unified ABC (c5p), BlobStore enhancement (xf3), composition refactor (5lm), storage_mode passthrough (2sf), docs update (fi4). UnifiedCache now composes BlobStore for all storage ops. Merged to dev at b635689.","labels":["phase-2","storage-mode"],"dependencies":[{"issue_id":"CACHE-i4b","depends_on_id":"CACHE-tkg","type":"blocks","created_at":"2026-02-10T12:18:17.704761-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-jts","title":"Implement get_with_metadata() on UnifiedCache","description":"Documented in API_REFERENCE.md L136 but not implemented. Currently users must call `get()` then `get_metadata()` separately — two metadata lookups and potentially two blob reads.\n\nShould return `(value, metadata_dict)` tuple in a single atomic operation.","design":"Combine the logic from `get()` and `get_metadata()` into one method that does a single `metadata_backend.get_entry()` call, loads the blob, and returns both the deserialized object and the metadata dict.","acceptance_criteria":"- [ ] `get_with_metadata(cache_key=None, **kwargs) -\u003e Optional[Tuple[Any, Dict]]` added to `UnifiedCache`\n- [ ] Returns `(value, metadata_dict)` tuple in a single operation\n- [ ] Returns `None` if entry not found\n- [ ] Single metadata lookup (not two separate calls)\n- [ ] Thread-safe (uses `self._lock`)\n- [ ] Test: Verify returns both value and metadata\n- [ ] Test: Verify returns None for missing entry\n- [ ] All existing tests pass (787 passed baseline)","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:01:16.9024586-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T12:32:08.0199197-05:00","closed_at":"2026-02-09T12:32:08.0199197-05:00","close_reason":"Feature complete: get_with_metadata() implemented with 5 tests. Performs single metadata lookup, returns (data, metadata) tuple. Merged to dev.","labels":["api","core","high"]}
{"id":"CACHE-key-normalization","title":"Fix cache key inconsistency for positional vs keyword args","description":"Logically equivalent function calls produce different cache keys: f(1, 2) and f(a=1, b=2) hash differently. There is a commented-out assertion in test_cache_key_consistency.py:L256 documenting this.\\n\\nThe fix: normalize args/kwargs in _create_cache_key() (or in the decorator layer) so that positional args are mapped to their parameter names using inspect.signature before hashing.","acceptance_criteria":"- f(1, 2) and f(a=1, b=2) produce the same cache key when decorated with @cached\\n- Uncomment and pass the assertion in test_cache_key_consistency.py:L256\\n- Existing cache key tests still pass (no regression for kwargs-only callers)\\n- Works with *args and **kwargs functions (graceful fallback)","status":"closed","priority":2,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T16:58:08.0481332-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T17:19:45.5196948-05:00","closed_at":"2026-02-07T17:19:45.5196948-05:00","close_reason":"Already fixed by _normalize_function_args (inspect.signature().bind()). Uncommented test assertion, removed stale TODO.","labels":["bug","decorator"]}
{"id":"CACHE-kkc","title":"Implement CACHENESS_* environment variable configuration","description":"API_REFERENCE.md L1050 documents 6 `CACHENESS_*` environment variables but config.py has zero `os.environ` or `os.getenv` calls. Environment variable configuration is essential for container/CI/CD environments where code changes aren't possible.","design":"1. In CacheConfig.__init__ or a factory function, read `os.environ` for each variable\n2. Priority: explicit constructor args \u003e env vars \u003e defaults\n3. Use `os.getenv()` with `None` default to detect \"not set\"\n4. Parse numeric values (MB, seconds, level) with appropriate error handling","acceptance_criteria":"- [ ] 6 `CACHENESS_*` environment variables read in config.py\n- [ ] `CACHENESS_DIR` sets cache directory\n- [ ] `CACHENESS_MAX_SIZE_MB` sets max cache size\n- [ ] `CACHENESS_DEFAULT_TTL_SECONDS` sets default TTL\n- [ ] `CACHENESS_BACKEND` sets metadata backend type\n- [ ] `CACHENESS_COMPRESSION_CODEC` sets compression codec\n- [ ] `CACHENESS_COMPRESSION_LEVEL` sets compression level\n- [ ] Env vars override defaults but are overridden by explicit config\n- [ ] No env vars set → behavior unchanged (backward compatible)\n- [ ] Test: Each env var is respected when set\n- [ ] Test: Explicit config overrides env vars\n- [ ] All existing tests pass (787 passed baseline)","status":"open","priority":4,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T13:02:58.134017-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T13:02:58.134017-05:00","labels":["api","config","low"]}
{"id":"CACHE-m3w","title":"Run ruff format/lint and ty type checking on entire codebase","description":"After completing all feature work, run comprehensive quality gates:\n- `uv run ruff format .` — Format all Python files\n- `uv run ruff check --fix .` — Lint with auto-fixes\n- `uv run ty` — Type check entire codebase\n\nThis ensures all new code passes formatting, linting, and type checking standards before final merge.","status":"open","priority":4,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T14:19:46.4714684-05:00","created_by":"radioflyer28","updated_at":"2026-02-08T14:19:46.4714684-05:00"}
{"id":"CACHE-mdd","title":"Investigate hanging test_size_limit_enforcement test","description":"The test `tests/test_core.py::TestCacheness::test_size_limit_enforcement` hangs indefinitely when run. The test creates a cache with a very small size limit (0.005 MB) and puts 20 entries exceeding that limit, expecting LRU eviction to kick in.\n\n**Symptoms:**\n- Test runs indefinitely without completing\n- No errors or output - just hangs\n- Happens with JSONBackend metadata\n\n**Investigation needed:**\n1. Add comprehensive logging to _enforce_size_limit() and cleanup_by_size()\n2. Check if there's a lock contention issue (put() holds UnifiedCache._lock, calls _enforce_size_limit())\n3. Verify file_size is being tracked correctly in JSONBackend entries\n4. Check if _save_to_disk() in JSONBackend is causing issues when called repeatedly\n5. Consider if the test assumptions about entry sizes are correct\n\n**Temporary workaround:**\nTest is marked with `@pytest.mark.skip` to unblock work on CACHE-a7a.\n\n**Context:**\nThis was discovered while implementing cleanup_by_size() for CACHE-a7a. The backend parity test for cleanup_by_size passes, but this integration test hangs.","status":"closed","priority":2,"issue_type":"bug","assignee":"GitHub Copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-08T14:47:05.2458457-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T16:53:26.0716817-05:00","closed_at":"2026-02-09T16:53:26.0716817-05:00","close_reason":"Fixed: Two critical bugs in JsonBackend resolved - (1) Deadlock from non-reentrant lock when cleanup_by_size() calls get_stats(), fixed by using RLock. (2) Size miscalculation from rounding 0.00514MB to 0.01MB causing over-aggressive cleanup, fixed by removing round(). Test now passes: 813 passed, 70 skipped."}
{"id":"CACHE-tkg","title":"Storage Mode - Phase 1: storage_mode flag on UnifiedCache","description":"Add storage_mode config flag to UnifiedCache that disables cache-specific behaviors (TTL, eviction, stats, auto-delete), aliases hash_key, and adds content-addressable key generation. Enables using the cache system as a pure storage backend.","status":"closed","priority":1,"issue_type":"epic","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:16:05.8076286-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:34:33.8853463-05:00","closed_at":"2026-02-11T13:34:33.8853463-05:00","close_reason":"Phase 1 complete: storage_mode flag, hash_key alias, content_key, 31 tests, docs. All 10 sub-tasks closed.","labels":["phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg","depends_on_id":"CACHE-fpg","type":"blocks","created_at":"2026-02-10T12:21:01.6745704-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.1","title":"Add storage_mode flag to CacheConfig","description":"Add storage_mode: bool = False to top-level CacheConfig dataclass in src/cacheness/config.py. When True, override default_ttl_seconds to None, max_cache_size_mb to None, and cleanup_on_init to False in __post_init__.","status":"closed","priority":1,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:16:54.5034318-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:17:21.5508042-05:00","closed_at":"2026-02-11T13:17:21.5508042-05:00","close_reason":"Added storage_mode: bool = False field + __init__ param + overrides (TTL=None, max_size=None, cleanup=False, stats=False, auto_cleanup=False)","labels":["config","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.1","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:16:54.5049684-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.10","title":"Add tests for storage mode","description":"Test cases: storage_mode=True config initializes without errors; None TTL entries never expire, no cleanup on init; None max size entries never evicted; hash_key alias works in put()/get()/delete()/exists(); content-addressable same data=same key, different data=different key; no auto-delete on deserialization errors in storage mode; stats not tracked in storage mode.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:18:12.6637953-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:34:27.241978-05:00","closed_at":"2026-02-11T13:34:27.241978-05:00","close_reason":"31 tests covering config, TTL, eviction, stats, auto-delete, hash_key, content_key, end-to-end","labels":["phase-1","storage-mode","tests"],"dependencies":[{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:18:12.6835806-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg.3","type":"blocks","created_at":"2026-02-10T12:18:12.6932429-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg.4","type":"blocks","created_at":"2026-02-10T12:18:12.6951414-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg.5","type":"blocks","created_at":"2026-02-10T12:18:12.6969127-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg.6","type":"blocks","created_at":"2026-02-10T12:18:12.6990687-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg.7","type":"blocks","created_at":"2026-02-10T12:18:12.7006561-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.10","depends_on_id":"CACHE-tkg.8","type":"blocks","created_at":"2026-02-10T12:18:12.7028308-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.2","title":"Fix default_ttl_seconds=None validation","description":"In CacheMetadataConfig.__post_init__() at config.py L96, guard the \u003c= 0 check with 'if self.default_ttl_seconds is not None:'. Also update validate_config() if it has redundant validation paths. Currently None causes TypeError.","status":"closed","priority":1,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:18.7635046-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:17:27.8713531-05:00","closed_at":"2026-02-11T13:17:27.8713531-05:00","close_reason":"Guarded default_ttl_seconds validation: None now accepted (infinite TTL)","labels":["config","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.2","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:18.7658387-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.2","depends_on_id":"CACHE-tkg.1","type":"blocks","created_at":"2026-02-10T12:17:18.7692213-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.3","title":"Fix _cleanup_expired() for None TTL","description":"In core.py ~L829, add early return 'if ttl_seconds is None: return' before calling self.metadata_backend.cleanup_expired(ttl_seconds). No TTL means no cleanup. Currently crashes when TTL is None.","status":"closed","priority":1,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:24.4595057-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:17:32.9289918-05:00","closed_at":"2026-02-11T13:17:32.9289918-05:00","close_reason":"Added None TTL guards in _cleanup_expired() and _is_expired() after _DEFAULT_TTL resolution","labels":["core","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.3","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:24.4609398-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.3","depends_on_id":"CACHE-tkg.2","type":"blocks","created_at":"2026-02-10T12:17:24.4663041-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.4","title":"Fix _enforce_size_limit() for None max size","description":"In core.py ~L1826, add early return 'if self.config.storage.max_cache_size_mb is None: return'. Currently comparing float to None raises TypeError, breaking all put() calls when max_cache_size_mb is None.","status":"closed","priority":1,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:30.4371472-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:17:38.5978011-05:00","closed_at":"2026-02-11T13:17:38.5978011-05:00","close_reason":"Added early return in _enforce_size_limit() when max_cache_size_mb is None","labels":["core","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.4","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:30.43883-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.4","depends_on_id":"CACHE-tkg.1","type":"blocks","created_at":"2026-02-10T12:17:30.4417913-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.5","title":"Disable stats tracking in storage mode","description":"In get() at core.py, wrap increment_hits()/increment_misses() calls with 'if not self.config.storage_mode:'. Alternatively, gate on the existing enable_cache_stats flag that currently isn't checked in get(). Stats are a cache-only concept.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:37.1284757-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:17:46.2664321-05:00","closed_at":"2026-02-11T13:17:46.2664321-05:00","close_reason":"Added _record_hit()/_record_miss() helpers gated on enable_cache_stats; replaced all 19 direct increment_hits/misses calls","labels":["core","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.5","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:37.1471365-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.5","depends_on_id":"CACHE-tkg.1","type":"blocks","created_at":"2026-02-10T12:17:37.1569854-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.6","title":"Disable auto-delete on errors in storage mode","description":"In the except blocks of get() at core.py ~L1110-L1125, add 'if not self.config.storage_mode:' guard around self.delete(cache_key) calls. Storage users should not lose data due to transient deserialization issues. Keep transient IO error handling as-is.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:43.9483138-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:17:53.4915359-05:00","closed_at":"2026-02-11T13:17:53.4915359-05:00","close_reason":"Guarded 8 remove_entry calls in get()/get_with_metadata() with storage_mode check (integrity, signature, unsigned, generic errors). FileNotFoundError still cleans up. Verification still runs in storage_mode - only auto-deletion is suppressed.","labels":["core","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.6","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:43.9505676-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.6","depends_on_id":"CACHE-tkg.1","type":"blocks","created_at":"2026-02-10T12:17:43.9544815-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.7","title":"Add hash_key alias for public API","description":"Add hash_key property/parameter alias in public-facing methods: put() returns hash_key in result, get()/get_metadata()/delete()/exists() accept hash_key as alias for cache_key parameter. Implement via small helper that normalizes hash_key to cache_key internally. cache_key remains canonical internally.","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:50.8741251-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:34:25.9974574-05:00","closed_at":"2026-02-11T13:34:25.9974574-05:00","close_reason":"hash_key alias added to all 9 public methods via _resolve_hash_key_alias helper","labels":["api","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.7","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:50.8768705-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.7","depends_on_id":"CACHE-tkg.1","type":"blocks","created_at":"2026-02-10T12:17:50.8846869-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.8","title":"Add content-addressable key generation to UnifiedCache","description":"Port BlobStore._compute_content_hash() logic (SHA-256 based, 16 chars) into UnifiedCache as alternative key generation strategy. Add content_addressable: bool = False to config. When enabled, _create_cache_key() hashes data content instead of function parameters. Enables deduplication: same data = same key.","status":"closed","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:17:58.1726316-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:34:26.407589-05:00","closed_at":"2026-02-11T13:34:26.407589-05:00","close_reason":"content_key() static method ported from BlobStore (SHA-256, 16-char hex)","labels":["core","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.8","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:17:58.1742493-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.8","depends_on_id":"CACHE-tkg.1","type":"blocks","created_at":"2026-02-10T12:17:58.1788673-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-tkg.9","title":"Update docs for storage mode","description":"Update docs/BLOB_STORE.md comparison table to note UnifiedCache now supports storage mode. Add 'Storage Mode' section to docs/CONFIGURATION.md explaining storage_mode=True usage and what it disables (TTL, eviction, stats, auto-delete). Document hash_key alias and content-addressable mode.","status":"closed","priority":2,"issue_type":"task","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-10T12:18:05.3620216-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T13:34:26.8204611-05:00","closed_at":"2026-02-11T13:34:26.8204611-05:00","close_reason":"Added Storage Mode section to CONFIGURATION.md, updated BLOB_STORE.md comparison table","labels":["docs","phase-1","storage-mode"],"dependencies":[{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg","type":"parent-child","created_at":"2026-02-10T12:18:05.3641883-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg.3","type":"blocks","created_at":"2026-02-10T12:18:05.3674471-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg.4","type":"blocks","created_at":"2026-02-10T12:18:05.3696092-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg.5","type":"blocks","created_at":"2026-02-10T12:18:05.3719704-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg.6","type":"blocks","created_at":"2026-02-10T12:18:05.3742905-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg.7","type":"blocks","created_at":"2026-02-10T12:18:05.3770075-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-tkg.9","depends_on_id":"CACHE-tkg.8","type":"blocks","created_at":"2026-02-10T12:18:05.3791502-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-xf3","title":"Enhance BlobStore with xxhash, integrity, signing, custom metadata","description":"Add to BlobStore: xxhash key generation (option alongside uuid/SHA-256), integrity verification (verify_integrity()), cryptographic signing (CacheEntrySigner), config-aware HandlerRegistry, and SQLAlchemy custom metadata support. BlobStore should have feature parity with UnifiedCache's storage capabilities.","status":"closed","priority":2,"issue_type":"task","assignee":"copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-11T16:56:02.1948809-05:00","created_by":"radioflyer28","updated_at":"2026-02-11T18:19:51.0670372-05:00","closed_at":"2026-02-11T18:19:51.0670372-05:00","close_reason":"Implemented: xxhash keys, file hashing, entry signing, verify_integrity, config-aware handlers, thread safety. 36 new tests. Merged to dev at cd1edb6.","labels":["phase-2","refactor","storage-mode"],"dependencies":[{"issue_id":"CACHE-xf3","depends_on_id":"CACHE-c5p","type":"blocks","created_at":"2026-02-11T16:56:13.1267419-05:00","created_by":"radioflyer28"},{"issue_id":"CACHE-xf3","depends_on_id":"CACHE-i4b","type":"parent-child","created_at":"2026-02-11T16:56:13.2317681-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-yw9","title":"Fault Injection Tests for I/O and Crash Safety","description":"Add fault injection tests using unittest.mock.patch to verify Cacheness handles I/O failures, mid-operation crashes, and data corruption correctly. Currently zero fault injection tests exist.\n\nKnown failure modes to test:\n\n1. Orphaned blob on put() crash:\n   - Mock metadata_backend.put_entry() to raise after handler.put() succeeds\n   - Verify: blob file should be cleaned up (currently it is NOT — fix needed)\n   - Verify: no partial metadata entry exists\n\n2. get() auto-deletion on transient I/O errors:\n   - Mock open()/os.read() to raise IOError on first call, succeed on retry\n   - Verify: metadata entry should NOT be deleted (currently it IS — fix needed)\n   - Verify: blob file still exists on disk\n\n3. get() auto-deletion on handler exception:\n   - Mock handler.get() to raise ValueError\n   - Verify: behavior is intentional vs accidental deletion\n\n4. JSON backend corruption recovery:\n   - Write garbage bytes to JSON metadata file\n   - Verify: backend logs warning and starts fresh (current behavior)\n   - Verify: no crash or unhandled exception\n   - Consider: should it back up the corrupted file first?\n\n5. Disk full during blob write:\n   - Mock open().write() to raise OSError(errno.ENOSPC)\n   - Verify: no partial blob file left behind\n   - Verify: no metadata entry created\n\n6. Concurrent file deletion during get():\n   - Mock os.path.exists() to return True, then FileNotFoundError on open()\n   - Verify: metadata cleanup happens, no crash\n\nSubtasks:\n- [ ] Create tests/test_fault_injection.py\n- [ ] Test: orphaned blob on put() metadata failure\n- [ ] Test: get() transient IOError should not delete entry\n- [ ] Test: get() handler exception behavior\n- [ ] Test: JSON metadata file corruption recovery\n- [ ] Test: disk full during blob write\n- [ ] Test: TOCTOU race in get() (exists → open)\n- [ ] Fix put() to clean up blob on metadata write failure\n- [ ] Fix get() to distinguish transient vs permanent errors","design":"Use unittest.mock.patch to inject faults at I/O boundaries. No custom infrastructure needed — all tests run in standard pytest. Tests should verify both the current (possibly broken) behavior AND include markers for expected fixes. Use @pytest.mark.xfail for tests that document bugs to be fixed. Estimated effort: 3-5 days (includes 2 bug fixes in put/get).","acceptance_criteria":"- At least 5 fault injection test cases covering put() crash, get() transient errors, JSON corruption\n- put() cleans up blob on post-write exception\n- get() does NOT delete entry on transient IOError\n- JSON backend recovers gracefully from corruption with clear logging\n- All tests use unittest.mock.patch — no custom infrastructure needed\n- No regressions in existing 696-test baseline","status":"closed","priority":1,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:05:27.9061402-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T15:26:42.4676699-05:00","closed_at":"2026-02-07T15:26:42.4676699-05:00","close_reason":"17 fault injection tests added. 3 bugs fixed: put() orphaned blob cleanup, get() transient IOError preservation, JsonBackend wrong-schema handling.","labels":["bug","reliability","testing"],"dependencies":[{"issue_id":"CACHE-yw9","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:05:27.9093536-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-z6i","title":"Fix pre-commit hook: set -e kills Phase 1, install broken in worktrees","description":"Pre-commit hook uses `set -e` but Phase 1 ruff commands (lines 11-12) lack `|| true` guards. If `ruff check --fix` finds unfixable issues, it exits non-zero and kills the hook before Phase 2 runs — despite Phase 1 being designed to \"never fail.\" Also, install-hooks.ps1 hardcodes `.git\\hooks` which doesn't work in worktrees (worktrees use `.git` as a file, not a directory). Fix: (a) add `|| true` to Phase 1 commands, (b) use `git rev-parse --git-dir` in install scripts for worktree compatibility.","status":"closed","priority":2,"issue_type":"bug","assignee":"copilot","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-12T12:18:49.5029456-05:00","created_by":"radioflyer28","updated_at":"2026-02-12T12:21:31.6115709-05:00","closed_at":"2026-02-12T12:21:31.6115709-05:00","close_reason":"Fixed: removed `set -e` from pre-commit hook, added `|| true` to Phase 1 commands. Install scripts now use `git rev-parse --git-dir` for worktree compatibility. Verified hook works without --no-verify. Pushed to dev at aff6111.","labels":["dx","tooling"]}
{"id":"CACHE-zf7","title":"Standardize file size units and eliminate rounding errors","description":"**Problem:** Mixing KB, MB, bytes, and rounding at wrong layers causes bugs. CACHE-mdd revealed that `round(0.00514, 2) = 0.01` MB made cache appear 2x larger, causing cleanup to delete all entries.\n\n**Current issues:**\n1. get_stats() rounded total_size_mb for display → broke calculations\n2. Mix of bytes (file_size), MB (config limits), conversions scattered everywhere\n3. cleanup_by_size() converts MB→bytes→MB multiple times\n4. No clear convention for when to round vs use precise values\n\n**Examples of confusion:**\n- `file_size` stored in bytes (entry field)\n- `max_cache_size_mb` in MB (config)\n- `total_size_mb` rounded to 2 decimals (was breaking cleanup)\n- Size comparisons convert back and forth\n\n**Acceptance criteria:**\n- [ ] Size utility functions created and tested\n- [ ] get_stats() returns both precise and rounded sizes\n- [ ] cleanup_by_size() uses bytes internally, no mid-calculation rounding\n- [ ] All size conversions use centralized utilities\n- [ ] Tests verify no rounding errors in size-based operations\n- [ ] Documentation clarifies internal (bytes) vs API (MB) convention\n- [ ] Existing tests pass (813 baseline)","design":"**Phase 1 - Add size utilities (non-breaking):**\n```python\n# In new utils.py or existing module\ndef mb_to_bytes(size_mb: float) -\u003e int:\n    return int(size_mb * 1024 * 1024)\n\ndef bytes_to_mb(size_bytes: int) -\u003e float:\n    return size_bytes / (1024 * 1024)\n\ndef bytes_to_mb_rounded(size_bytes: int, decimals: int = 2) -\u003e float:\n    return round(bytes_to_mb(size_bytes), decimals)\n```\n\n**Phase 2 - Fix get_stats() (already done):**\n- Return precise `total_size_mb` for calculations\n- Add `total_size_mb_display` with rounding for UI\n\n**Phase 3 - Audit and fix conversions:**\n- grep for `* 1024 * 1024` and `/ (1024 * 1024)`\n- Replace with utility functions\n- Ensure cleanup_by_size() uses bytes internally\n\n**Phase 4 - Consider schema change (breaking):**\n- Change `file_size` column to always be bytes (int)\n- Update SQLite/JSON backends\n- Migration for existing caches\n\n**Benefits:**\n- Eliminates floating-point rounding errors\n- Clear separation: internal (bytes) vs external (MB)\n- Easier to reason about size calculations\n- Less error-prone conversion logic","status":"open","priority":3,"issue_type":"chore","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-09T16:56:09.3833471-05:00","created_by":"radioflyer28","updated_at":"2026-02-09T16:56:09.3833471-05:00"}

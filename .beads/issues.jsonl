{"id":"CACHE-6j7","title":"Advanced Testing Framework (TigerBeetle-Inspired)","description":"Adopt targeted testing techniques inspired by TigerBeetle's testing philosophy — \"prove correctness, don't just demonstrate it\" — adapted to Cacheness's risk profile as a single-process caching library.\n\nAnalysis identified 7 untested failure modes:\n- _lock never acquired — put() not thread-safe at coordination layer\n- Same-key concurrent put() can corrupt metadata or produce orphaned blobs\n- Crash between blob write and metadata write → orphaned blobs\n- No blob cleanup on put() exception after blob is written\n- get() auto-deletes entries on transient I/O errors\n- get() auto-deletes on any handler exception\n- JSON backend silently discards all data on corruption\n\nFour workstreams address these gaps (~8-12 days total effort):\n1. Property-based testing with Hypothesis (P1)\n2. Fault injection via mocking (P1)\n3. Invariant checks / fsck() method (P2)\n4. Fix _lock + concurrency stress tests (P2)\n\nSkipped (cost too high for caching library risk profile):\n- Deterministic simulation (VOPR-style) — overkill for single-process lib\n- Continuous fuzzing infrastructure (CFO) — blast radius too low to justify\n\nReference: https://deepwiki.com/tigerbeetle/tigerbeetle/5.2-testing-and-simulation","status":"open","priority":2,"issue_type":"epic","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:04:43.126371-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T14:04:43.126371-05:00","labels":["reliability","testing"]}
{"id":"CACHE-78r","title":"Property-Based Testing with Hypothesis","description":"Add property-based testing using Hypothesis to find edge cases in serialization round-trips, cache key determinism, and metadata backend contracts. Hypothesis is already mentioned in dev deps but is not installed or used anywhere.\n\nCurrent state: 0 property-based tests. All 696 tests are example-based.\n\nTargets for @given decorators:\n\n1. Handler round-trip invariant:\n   - handler.put(obj, path) → handler.get(path) === obj\n   - Test with random DataFrames (varying dtypes, shapes, NaN, empty)\n   - Test with random NumPy arrays (varying dtypes, shapes, edge values)\n   - Test with random Python objects via PickleHandler\n\n2. Cache key determinism:\n   - _create_cache_key(**kwargs) must be deterministic (same input → same key)\n   - Test with random kwargs dicts (strings, ints, floats, nested)\n   - Verify named params (prefix, description, custom_metadata) don't affect key\n\n3. Metadata backend contract:\n   - put_entry(key, data) → get_entry(key) returns equivalent data\n   - put_entry → delete_entry → get_entry returns None\n   - list_entries contains all put keys\n   - Test across JSON, SQLite, and sqlite_memory backends\n\n4. Serialization round-trips:\n   - compress_pickle: compress(data) → decompress() === data\n   - Test with random bytes, varying sizes, all codec combinations\n\nSubtasks:\n- [ ] Install hypothesis: uv add --dev hypothesis\n- [ ] Create tests/test_property_based.py\n- [ ] Handler round-trip tests with st.one_of for multiple types\n- [ ] Cache key determinism tests\n- [ ] Metadata backend contract tests (parametrized across backends)\n- [ ] Compression round-trip tests across codecs\n- [ ] Add to CI (hypothesis tests can be slow — use deadline setting)","design":"Create tests/test_property_based.py with @given decorators targeting handler round-trips, cache key determinism, metadata backend contracts, and compression round-trips. Use hypothesis strategies (st.dictionaries, st.floats, st.binary, st.text) to generate random inputs. Parametrize backend tests across json/sqlite/sqlite_memory. Set deadline=None for slow tests. Estimated effort: 2-3 days.","acceptance_criteria":"- hypothesis installed as dev dependency\n- At least 4 property-based test classes covering handlers, cache keys, backends, compression\n- All property tests pass with default hypothesis settings\n- No regressions in existing 696-test baseline","status":"open","priority":1,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:05:07.3259734-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T14:05:07.3259734-05:00","labels":["reliability","testing"],"dependencies":[{"issue_id":"CACHE-78r","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:05:07.333393-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-8fu","title":"Cache Integrity Verification (fsck) Method","description":"Add a production-usable verify_integrity() / fsck() method to UnifiedCache that detects and optionally repairs inconsistencies between blob files and metadata. Inspired by TigerBeetle's StateChecker pattern — lightweight invariant validation that can run in production, not just tests.\n\nCurrent state: No integrity verification exists. Users have no way to detect orphaned blobs, dangling metadata entries, or size mismatches.\n\nInvariants to check:\n\n1. No orphaned blobs:\n   - Every file in cache_dir should have a corresponding metadata entry\n   - Files without metadata = orphaned (wasted disk space, invisible)\n\n2. No dangling metadata:\n   - Every metadata entry should point to an existing blob file\n   - Entries without files = dangling (get() will fail and auto-delete)\n\n3. Size consistency:\n   - metadata.file_size should match actual os.path.getsize()\n   - Mismatch suggests truncated write or external modification\n\n4. Hash consistency (optional, expensive):\n   - metadata.file_hash should match actual file hash\n   - Detects corruption or tampering\n\nAPI design:\n```python\ncache = cacheness()\n\n# Check only (no modifications)\nreport = cache.verify_integrity()\n# Returns: {\"orphaned_blobs\": [...], \"dangling_entries\": [...], \"size_mismatches\": [...]}\n\n# Check and repair\nreport = cache.verify_integrity(repair=True)\n# Deletes orphaned blobs, removes dangling entries\n```\n\nSubtasks:\n- [ ] Add verify_integrity(repair=False) method to UnifiedCache\n- [ ] Implement orphaned blob detection (scan cache_dir vs metadata keys)\n- [ ] Implement dangling metadata detection (metadata entries vs actual files)\n- [ ] Implement size mismatch detection\n- [ ] Optional: hash verification (gated by verify_hashes=True flag)\n- [ ] Add repair mode (clean up orphans and dangling entries)\n- [ ] Create tests/test_cache_integrity_verification.py\n- [ ] Document in API_REFERENCE.md","design":"Add verify_integrity() to UnifiedCache that scans both cache_dir (blob files) and metadata backend (entries) to find inconsistencies. Use iter_entry_summaries() for metadata scan (lightweight). Walk cache_dir for blob inventory. Compare sets to find orphans and dangles. Return structured dict report. Optional repair mode deletes orphans and removes dangling entries. Estimated effort: 1-2 days.","acceptance_criteria":"- verify_integrity() method on UnifiedCache returns structured report\n- Detects orphaned blobs (files with no metadata entry)\n- Detects dangling metadata (entries pointing to missing files)\n- Detects size mismatches between metadata and actual file\n- Optionally repairs (clean=True deletes orphans, removes dangling entries)\n- Tests cover all detected anomaly types\n- No regressions in existing 696-test baseline","status":"open","priority":2,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:05:47.4150238-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T14:05:47.4150238-05:00","labels":["reliability","testing"],"dependencies":[{"issue_id":"CACHE-8fu","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:05:47.4180242-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-a6a","title":"Fix _lock Thread Safety + Concurrency Stress Tests","description":"Fix the thread safety gap in UnifiedCache and add proper concurrency stress tests. Currently self._lock is created at __init__ (line 87 of core.py) but NEVER acquired anywhere — making all UnifiedCache methods completely unsynchronized at the coordination layer.\n\nThe existing concurrency tests in test_sqlite_concurrency.py use unique keys per thread, which avoids the actual race condition. They also accept \"database is locked\" errors as OK.\n\nPart 1 — Fix the lock:\n- Acquire self._lock in put(), get(), update_data(), invalidate(), delete_where(), touch(), and other methods that perform multi-step blob+metadata operations\n- The lock protects the blob-write → metadata-write sequence from interleaving\n- Backend-level locks (JSON Lock, SQLite RLock) still protect individual backend operations\n- Use context manager: with self._lock: for clean acquisition/release\n\nPart 2 — Add same-key concurrency tests:\n- N threads all calling put() with the SAME cache key simultaneously\n- Verify: exactly 1 entry exists after all threads complete\n- Verify: no orphaned blobs\n- Verify: the stored data matches one of the thread's inputs\n\nPart 3 — Add cross-operation concurrency tests:\n- Threads mixing put(), get(), invalidate() on overlapping keys\n- Use threading.Barrier to maximize collision probability\n- Test across JSON, SQLite, and sqlite_memory backends (current tests only cover SQLite)\n\nPart 4 — Tighten existing concurrency tests:\n- Remove acceptance of \"database is locked\" as OK — these should be retried or prevented\n- Add proper assertions instead of filtering out errors\n\nSubtasks:\n- [ ] Acquire self._lock in put() around blob-write + metadata-write sequence\n- [ ] Acquire self._lock in get() around metadata-read + blob-read sequence\n- [ ] Acquire self._lock in update_data(), invalidate(), touch()\n- [ ] Add same-key concurrent put() test\n- [ ] Add put()/get()/invalidate() interleaving test\n- [ ] Add JSON backend concurrency test\n- [ ] Add sqlite_memory concurrency test\n- [ ] Tighten existing test_sqlite_concurrency.py assertions\n- [ ] Verify no deadlocks with backend-level locks (JSON, SQLite)","design":"Fix is straightforward: wrap multi-step operations in with self._lock:. The lock scope should be the entire put() or get() call — not just the metadata portion — because we need atomicity of the blob+metadata pair. Tests use threading.Barrier to synchronize thread start for maximum collision probability. Parametrize across backends. Estimated effort: 1-2 days.","acceptance_criteria":"- self._lock is acquired in put(), get(), update_data(), invalidate()\n- Same-key concurrent put() test passes (N threads, 1 key, no corruption)\n- Concurrent put()/get() interleaving test passes\n- Concurrent tests added for JSON and sqlite_memory backends (not just SQLite)\n- Existing concurrency tests still pass\n- No regressions in existing 696-test baseline","status":"open","priority":2,"issue_type":"bug","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:06:09.5968365-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T14:06:09.5968365-05:00","labels":["bug","concurrency","reliability","testing"],"dependencies":[{"issue_id":"CACHE-a6a","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:06:09.6000521-05:00","created_by":"radioflyer28"}]}
{"id":"CACHE-yw9","title":"Fault Injection Tests for I/O and Crash Safety","description":"Add fault injection tests using unittest.mock.patch to verify Cacheness handles I/O failures, mid-operation crashes, and data corruption correctly. Currently zero fault injection tests exist.\n\nKnown failure modes to test:\n\n1. Orphaned blob on put() crash:\n   - Mock metadata_backend.put_entry() to raise after handler.put() succeeds\n   - Verify: blob file should be cleaned up (currently it is NOT — fix needed)\n   - Verify: no partial metadata entry exists\n\n2. get() auto-deletion on transient I/O errors:\n   - Mock open()/os.read() to raise IOError on first call, succeed on retry\n   - Verify: metadata entry should NOT be deleted (currently it IS — fix needed)\n   - Verify: blob file still exists on disk\n\n3. get() auto-deletion on handler exception:\n   - Mock handler.get() to raise ValueError\n   - Verify: behavior is intentional vs accidental deletion\n\n4. JSON backend corruption recovery:\n   - Write garbage bytes to JSON metadata file\n   - Verify: backend logs warning and starts fresh (current behavior)\n   - Verify: no crash or unhandled exception\n   - Consider: should it back up the corrupted file first?\n\n5. Disk full during blob write:\n   - Mock open().write() to raise OSError(errno.ENOSPC)\n   - Verify: no partial blob file left behind\n   - Verify: no metadata entry created\n\n6. Concurrent file deletion during get():\n   - Mock os.path.exists() to return True, then FileNotFoundError on open()\n   - Verify: metadata cleanup happens, no crash\n\nSubtasks:\n- [ ] Create tests/test_fault_injection.py\n- [ ] Test: orphaned blob on put() metadata failure\n- [ ] Test: get() transient IOError should not delete entry\n- [ ] Test: get() handler exception behavior\n- [ ] Test: JSON metadata file corruption recovery\n- [ ] Test: disk full during blob write\n- [ ] Test: TOCTOU race in get() (exists → open)\n- [ ] Fix put() to clean up blob on metadata write failure\n- [ ] Fix get() to distinguish transient vs permanent errors","design":"Use unittest.mock.patch to inject faults at I/O boundaries. No custom infrastructure needed — all tests run in standard pytest. Tests should verify both the current (possibly broken) behavior AND include markers for expected fixes. Use @pytest.mark.xfail for tests that document bugs to be fixed. Estimated effort: 3-5 days (includes 2 bug fixes in put/get).","acceptance_criteria":"- At least 5 fault injection test cases covering put() crash, get() transient errors, JSON corruption\n- put() cleans up blob on post-write exception\n- get() does NOT delete entry on transient IOError\n- JSON backend recovers gracefully from corruption with clear logging\n- All tests use unittest.mock.patch — no custom infrastructure needed\n- No regressions in existing 696-test baseline","status":"open","priority":1,"issue_type":"feature","owner":"akgithub.2drwc@aleeas.com","created_at":"2026-02-07T14:05:27.9061402-05:00","created_by":"radioflyer28","updated_at":"2026-02-07T14:05:27.9061402-05:00","labels":["bug","reliability","testing"],"dependencies":[{"issue_id":"CACHE-yw9","depends_on_id":"CACHE-6j7","type":"blocks","created_at":"2026-02-07T14:05:27.9093536-05:00","created_by":"radioflyer28"}]}
